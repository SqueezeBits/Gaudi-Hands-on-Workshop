{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6d59bb",
   "metadata": {},
   "source": [
    "# [A-3] Running Quantization and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b44d80",
   "metadata": {},
   "source": [
    "*Make sure to restart the Kernel before executing this notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd184d88",
   "metadata": {},
   "source": [
    "In this notebook, we will quantize the model and finally run inference with the quantized model. Let's see how the performance get improved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae4a85",
   "metadata": {},
   "source": [
    "Similarly, set model cache path and `QUANT_CONFIG` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a246391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"QUANT_CONFIG\"] = f\"{os.getcwd()}/configs/quantize.json\"\n",
    "MODEL_PATH = \"Qwen/Qwen3-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4c52b3",
   "metadata": {},
   "source": [
    "This time, we should use the config file for quantization. You can see the mode is set to `QUANTIZE`. For this walkthrough we will use `maxabs_hw` scaling method. For the available scaling methods, check the details [here](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Quantization/Inference_Using_FP8.html?highlight=pt_hpu_weight_sharing#supported-json-config-file-options)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "11a1bca4",
   "metadata": {},
   "source": [
    "{\n",
    "    \"mode\": \"QUANTIZE\",\n",
    "    \"observer\": \"maxabs\",\n",
    "    \"scale_method\": \"maxabs_hw\",\n",
    "    \"allowlist\": {\n",
    "        \"types\": [],\n",
    "        \"names\": []\n",
    "    },\n",
    "    \"blocklist\": {\n",
    "        \"types\": [],\n",
    "        \"names\": []\n",
    "    },\n",
    "    \"dump_stats_path\": \"calibration_outputs/inc_output\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120f5bcc",
   "metadata": {},
   "source": [
    "Note that model is not quantized yet. Quantization is performed while initializing the LLM instance. The statistics collected during the calibration phase will be loaded to apply quantization.\n",
    "\n",
    "As we've seen in the previous notebook, `quantize` API will be applied to the model while loading. The patched modules for `QUANTIZE` mode perform quantization on input tensors and utilize the low precision kernels. While the additional quantize and dequantize operations may seem to introduce type casting overhead, SynapseAI's graph compiler optimizes them by eliminating unnecessary consecutive QDQ operations whenever possible, ensuring that activation tensors remain in low precision throughout the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48190844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "os.environ[\"PT_HPU_WEIGHT_SHARING\"] = \"0\"\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.0)\n",
    "llm = LLM(model=MODEL_PATH, quantization=\"inc\", kv_cache_dtype=\"fp8_inc\")\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51479ea7",
   "metadata": {},
   "source": [
    "You may see the number of available blocks increased compared to the baseline\n",
    "\n",
    "> [hpu_worker.py:243] Usable num_blocks: 7855 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47410c69",
   "metadata": {},
   "source": [
    "Now, Define the same benchmarking helper as in the first notebook to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab80ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.benchmarks.datasets import SampleRequest\n",
    "from vllm.inputs import TextPrompt, TokensPrompt\n",
    "from vllm.lora.request import LoRARequest\n",
    "from vllm.outputs import RequestOutput\n",
    "from vllm.sampling_params import BeamSearchParams\n",
    "\n",
    "\n",
    "def run_vllm(\n",
    "    llm: LLM,\n",
    "    requests: list[SampleRequest],\n",
    "    n: int,\n",
    "    do_profile: bool,\n",
    "    disable_detokenize: bool = False,\n",
    ") -> tuple[float, list[RequestOutput] | None]:\n",
    "    assert all(\n",
    "        llm.llm_engine.model_config.max_model_len\n",
    "        >= (request.prompt_len + request.expected_output_len)\n",
    "        for request in requests\n",
    "    ), (\n",
    "        \"Please ensure that max_model_len is greater than the sum of\"\n",
    "        \" prompt_len and expected_output_len for all requests.\"\n",
    "    )\n",
    "    # Add the requests to the engine.\n",
    "    prompts: list[TextPrompt | TokensPrompt] = []\n",
    "    sampling_params: list[SamplingParams] = []\n",
    "    for request in requests:\n",
    "        prompt = (\n",
    "            TokensPrompt(prompt_token_ids=request.prompt[\"prompt_token_ids\"])\n",
    "            if \"prompt_token_ids\" in request.prompt\n",
    "            else TextPrompt(prompt=request.prompt)\n",
    "        )\n",
    "        if request.multi_modal_data:\n",
    "            assert isinstance(request.multi_modal_data, dict)\n",
    "            prompt[\"multi_modal_data\"] = request.multi_modal_data\n",
    "        prompts.append(prompt)\n",
    "\n",
    "        sampling_params.append(\n",
    "            SamplingParams(\n",
    "                n=n,\n",
    "                temperature=1.0,\n",
    "                top_p=1.0,\n",
    "                ignore_eos=True,\n",
    "                max_tokens=request.expected_output_len,\n",
    "                detokenize=not disable_detokenize,\n",
    "            )\n",
    "        )\n",
    "    lora_requests: list[LoRARequest] | None = None\n",
    "\n",
    "    use_beam_search = False\n",
    "\n",
    "    outputs = None\n",
    "    if not use_beam_search:\n",
    "        start = time.perf_counter()\n",
    "        if do_profile:\n",
    "            llm.start_profile()\n",
    "        outputs = llm.generate(\n",
    "            prompts, sampling_params, lora_request=lora_requests, use_tqdm=True\n",
    "        )\n",
    "        if do_profile:\n",
    "            llm.stop_profile()\n",
    "        end = time.perf_counter()\n",
    "    else:\n",
    "        assert lora_requests is None, \"BeamSearch API does not support LoRA\"\n",
    "        prompts = [request.prompt for request in requests]\n",
    "        # output_len should be the same for all requests.\n",
    "        output_len = requests[0].expected_output_len\n",
    "        for request in requests:\n",
    "            assert request.expected_output_len == output_len\n",
    "        start = time.perf_counter()\n",
    "        if do_profile:\n",
    "            llm.start_profile()\n",
    "        llm.beam_search(\n",
    "            prompts,\n",
    "            BeamSearchParams(\n",
    "                beam_width=n,\n",
    "                max_tokens=output_len,\n",
    "                ignore_eos=True,\n",
    "            ),\n",
    "        )\n",
    "        if do_profile:\n",
    "            llm.stop_profile()\n",
    "        end = time.perf_counter()\n",
    "    return end - start, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5729c40a",
   "metadata": {},
   "source": [
    "Now run the benchmark on the quantized model.\n",
    "\n",
    "Compare the **Throughput (tokens/s)** with the baseline FP16/BF16 results from the first notebook. You should observe a significant improvement in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.benchmarks.datasets import RandomDataset\n",
    "\n",
    "requests = RandomDataset(\n",
    "    dataset_path=None,\n",
    "    random_seed=42,\n",
    ").sample(\n",
    "    tokenizer=llm.get_tokenizer(),\n",
    "    input_len=512,\n",
    "    output_len=512,\n",
    "    num_requests=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time, request_outputs = run_vllm(\n",
    "    llm,\n",
    "    requests,\n",
    "    n=1,\n",
    "    do_profile=False,\n",
    ")\n",
    "\n",
    "total_prompt_tokens = 0\n",
    "total_output_tokens = 0\n",
    "for ro in request_outputs:\n",
    "    if not isinstance(ro, RequestOutput):\n",
    "        continue\n",
    "    total_prompt_tokens += (\n",
    "        len(ro.prompt_token_ids) if ro.prompt_token_ids else 0\n",
    "    )\n",
    "    total_output_tokens += sum(len(o.token_ids) for o in ro.outputs if o)\n",
    "total_num_tokens = total_prompt_tokens + total_output_tokens\n",
    "print(f\"Total num prompt tokens:  {total_prompt_tokens}\")\n",
    "print(f\"Total num output tokens:  {total_output_tokens}\")\n",
    "print(\n",
    "    f\"Throughput: {len(requests) / elapsed_time:.2f} requests/s, \"\n",
    "    f\"{total_num_tokens / elapsed_time:.2f} total tokens/s, \"\n",
    "    f\"{total_output_tokens / elapsed_time:.2f} output tokens/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb369aee",
   "metadata": {},
   "source": [
    "In addition to FP8 quantization, `vllm-gaudi` supports a variety of serving features, including MultiLoRA, Guided Decoding, Automatic Prefix Caching, and more. You can check the full list of supported features [here](https://docs.vllm.ai/projects/gaudi/en/latest/features/supported_features.html#supported-features_1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
