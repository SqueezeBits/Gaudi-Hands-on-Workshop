{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b067747",
   "metadata": {},
   "source": [
    "# [A-1] Running vLLM with Gaudi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b85a271",
   "metadata": {},
   "source": [
    "## Wath you will do\n",
    "In this walkthrough, we will run a simple vLLM inference and apply FP8 quantization on Intel® Gaudi® accelerators(HPU) with Intel® Neural Compressor.\n",
    "\n",
    "- **Stage 1**: run basic vLLM inference and benchmark a baseline model\n",
    "- **Stage 2**: run calibration and see how INC is integrated into `vllm-gaudi`\n",
    "- **Stage 3**: run quantiation and benchmark the quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0311747b",
   "metadata": {},
   "source": [
    "`vllm-gaudi` is built upon **vLLM Hardware Plugin**, which allows vLLM to utilize Gaudi devices seamlessly. Because of this integration, the Python interface remains almost the same as running vLLM on GPUs.\n",
    "\n",
    "> The vLLM Hardware Plugin for Intel® Gaudi® is a community-driven integration layer that enables efficient, high-performance large language model (LLM) inference on Intel® Gaudi® AI accelerators.\n",
    "> \n",
    "> The vLLM Hardware Plugin for Intel® Gaudi® connects the vLLM serving engine with Intel® Gaudi® hardware, offering optimized inference capabilities for enterprise-scale LLM workloads. It is developed and maintained by the Intel® Gaudi® team and follows the hardware pluggable RFC and vLLM plugin architecture RFC for modular integration.\n",
    "\n",
    "Someone might have seen a [folked repository](https://github.com/HabanaAI/vllm-fork) for Gaudi, which is the initial implementation of vLLM for Gaudi. As vLLM moves to v1, backend support is now recommended to be implemented via a plugin-based approach, which is why the transition to vllm-gaudi is underway. Since this is still a transitional phase, some advanced features may currently be available only in the forked repository. However, the fork is expected to be deprecated in the long term.\n",
    "\n",
    "`vllm-gaudi` is installed in your environment for this hands-on workshop. Check if it's properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b62e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep vllm_gaudi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4926f96a",
   "metadata": {},
   "source": [
    "## Running offline inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6431d0",
   "metadata": {},
   "source": [
    "Set the `HF_HUB_CACHE` environment variable properly to point to pre-downloaded hub path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452383e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/home/work/Qwen3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The future of AI is\",\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.0)\n",
    "llm = LLM(model=MODEL_PATH)\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02994e5",
   "metadata": {},
   "source": [
    "### KV Cache and Memory Blocks\n",
    "\n",
    "vLLM manages memory using **PagedAttention**, which allocates Key-Value (KV) cache in non-contiguous memory blocks. Check the logs for the number of usable blocks:\n",
    "\n",
    "> `[hpu_worker.py:243] Usable num_blocks: 3655 ...`\n",
    "\n",
    "This number is calculated based on the free HPU memory available after running profile run.\n",
    "*   **More free memory = More blocks = Larger batch size / context length capability.**\n",
    "\n",
    "Keep a note of this number. Later, when we use a **quantized model** (which is smaller), we expect to see a higher number of available blocks, allowing for even higher throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3eb48d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Graph Compilation and Warm-up\n",
    "\n",
    "During the initialization of the `llm` instance, you will observe \"warm-up\" logs similar to:\n",
    "> `Prompt warmup processing: 100%|██████████| 121/121`  \n",
    "> `Decode warmup processing: 100%|██████████| 37/37`\n",
    "\n",
    "`vllm-gaudi` implements `HPUBucketingManager` to precompile graphs for different input shapes. While this warm-up process introduces some overhead during runner initialization, it enables faster inference by ensuring proper graph handling without on-the-fly recompilation. You can disable the warm-up process by setting the environment variable `VLLM_SKIP_WARMUP=True`.\n",
    "\n",
    "As you can indicate from the compilation log, prefill graphs and decode graphs are being compiled separately. Although this exercise runs prefill and decode separately, `vllm-gaudi` has recently added support for mixed batch and unified attention. More details are available [here](https://github.com/vllm-project/vllm-gaudi/blob/v0.11.2/docs/features/unified_attn.md#unifiedmixed-batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115b649",
   "metadata": {},
   "source": [
    "## Running a benchmark with the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32014047",
   "metadata": {},
   "source": [
    "Now that the engine is running, let's establish a performance baseline.\n",
    "\n",
    "We will define a helper function, `run_vllm`, to execute a benchmark using the loaded engine. This function processes a list of requests and measures the execution time, which we will compare against the quantized model in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.benchmarks.datasets import SampleRequest\n",
    "from vllm.inputs import TextPrompt, TokensPrompt\n",
    "from vllm.lora.request import LoRARequest\n",
    "from vllm.outputs import RequestOutput\n",
    "from vllm.sampling_params import BeamSearchParams\n",
    "\n",
    "\n",
    "def run_vllm(\n",
    "    llm: LLM,\n",
    "    requests: list[SampleRequest],\n",
    "    n: int,\n",
    "    do_profile: bool,\n",
    "    disable_detokenize: bool = False,\n",
    ") -> tuple[float, list[RequestOutput] | None]:\n",
    "    assert all(\n",
    "        llm.llm_engine.model_config.max_model_len\n",
    "        >= (request.prompt_len + request.expected_output_len)\n",
    "        for request in requests\n",
    "    ), (\n",
    "        \"Please ensure that max_model_len is greater than the sum of\"\n",
    "        \" prompt_len and expected_output_len for all requests.\"\n",
    "    )\n",
    "    # Add the requests to the engine.\n",
    "    prompts: list[TextPrompt | TokensPrompt] = []\n",
    "    sampling_params: list[SamplingParams] = []\n",
    "    for request in requests:\n",
    "        prompt = (\n",
    "            TokensPrompt(prompt_token_ids=request.prompt[\"prompt_token_ids\"])\n",
    "            if \"prompt_token_ids\" in request.prompt\n",
    "            else TextPrompt(prompt=request.prompt)\n",
    "        )\n",
    "        if request.multi_modal_data:\n",
    "            assert isinstance(request.multi_modal_data, dict)\n",
    "            prompt[\"multi_modal_data\"] = request.multi_modal_data\n",
    "        prompts.append(prompt)\n",
    "\n",
    "        sampling_params.append(\n",
    "            SamplingParams(\n",
    "                n=n,\n",
    "                temperature=1.0,\n",
    "                top_p=1.0,\n",
    "                ignore_eos=True,\n",
    "                max_tokens=request.expected_output_len,\n",
    "                detokenize=not disable_detokenize,\n",
    "            )\n",
    "        )\n",
    "    lora_requests: list[LoRARequest] | None = None\n",
    "\n",
    "    use_beam_search = False\n",
    "\n",
    "    outputs = None\n",
    "    if not use_beam_search:\n",
    "        start = time.perf_counter()\n",
    "        if do_profile:\n",
    "            llm.start_profile()\n",
    "        outputs = llm.generate(\n",
    "            prompts, sampling_params, lora_request=lora_requests, use_tqdm=True\n",
    "        )\n",
    "        if do_profile:\n",
    "            llm.stop_profile()\n",
    "        end = time.perf_counter()\n",
    "    else:\n",
    "        assert lora_requests is None, \"BeamSearch API does not support LoRA\"\n",
    "        prompts = [request.prompt for request in requests]\n",
    "        \n",
    "        output_len = requests[0].expected_output_len\n",
    "        for request in requests:\n",
    "            assert request.expected_output_len == output_len\n",
    "        start = time.perf_counter()\n",
    "        if do_profile:\n",
    "            llm.start_profile()\n",
    "        llm.beam_search(\n",
    "            prompts,\n",
    "            BeamSearchParams(\n",
    "                beam_width=n,\n",
    "                max_tokens=output_len,\n",
    "                ignore_eos=True,\n",
    "            ),\n",
    "        )\n",
    "        if do_profile:\n",
    "            llm.stop_profile()\n",
    "        end = time.perf_counter()\n",
    "    return end - start, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset_desc",
   "metadata": {},
   "source": [
    "We will use `RandomDataset` to create a synthetic dataset.\n",
    "\n",
    "*   `input_len=512`: The number of tokens in the prompt.\n",
    "*   `output_len=512`: The number of tokens to generate.\n",
    "*   `num_requests=512`: Total number of requests to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.benchmarks.datasets import RandomDataset\n",
    "\n",
    "requests = RandomDataset(\n",
    "    dataset_path=None,\n",
    "    random_seed=42,\n",
    ").sample(\n",
    "    tokenizer=llm.get_tokenizer(),\n",
    "    input_len=512,\n",
    "    output_len=512,\n",
    "    num_requests=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bench_desc",
   "metadata": {},
   "source": [
    "Now we run the benchmark using our `run_vllm` helper. This will report:\n",
    "\n",
    "*   **Requests/s**: How many requests the server can handle per second.\n",
    "*   **Total tokens/s**: Including both prompt processing and generation.\n",
    "*   **Output tokens/s**: The speed of token generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time, request_outputs = run_vllm(\n",
    "    llm,\n",
    "    requests,\n",
    "    n=1,\n",
    "    do_profile=False,\n",
    ")\n",
    "\n",
    "total_prompt_tokens = 0\n",
    "total_output_tokens = 0\n",
    "for ro in request_outputs:\n",
    "    if not isinstance(ro, RequestOutput):\n",
    "        continue\n",
    "    total_prompt_tokens += (\n",
    "        len(ro.prompt_token_ids) if ro.prompt_token_ids else 0\n",
    "    )\n",
    "    total_output_tokens += sum(len(o.token_ids) for o in ro.outputs if o)\n",
    "total_num_tokens = total_prompt_tokens + total_output_tokens\n",
    "print(f\"Total num prompt tokens:  {total_prompt_tokens}\")\n",
    "print(f\"Total num output tokens:  {total_output_tokens}\")\n",
    "print(\n",
    "    f\"Throughput: {len(requests) / elapsed_time:.2f} requests/s, \"\n",
    "    f\"{total_num_tokens / elapsed_time:.2f} total tokens/s, \"\n",
    "    f\"{total_output_tokens / elapsed_time:.2f} output tokens/s\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
