{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2a286b",
   "metadata": {},
   "source": [
    "# [A-2] Running Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d9d775",
   "metadata": {},
   "source": [
    "*Make sure to restart the Kernel before executing this notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d7fcd",
   "metadata": {},
   "source": [
    "In this notebook, we will perform **calibration** for quantization. \n",
    "\n",
    "Calibration is a crucial step for Post-Training Quantization (PTQ). It involves running a representative dataset through the model to collect statistics (ranges) of activations and weights. These statistics are then used to calculate scale factors for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c60f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/home/work/Qwen3-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81baca",
   "metadata": {},
   "source": [
    "## Intel Neural Compressor (INC)\n",
    "\n",
    "[INC](https://github.com/intel/neural-compressor) is an open-source library for model compression developed by Intel. It provides implementations of calibration and quantization methods that can be easily applied to existing models. As it is developed primarily by Intel, it is highly optimized for Intel hardware, including the Gaudi series.\n",
    "\n",
    "INC is closely integrated into `vllm-gaudi`, allowing calibration and quantization to be applied seamlessly through the vLLM APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712db1b",
   "metadata": {},
   "source": [
    "Now, let's start running calibration first. We should specify configurations in a config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd34fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"QUANT_CONFIG\"] = f\"{os.getcwd()}/configs/measure.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba068831",
   "metadata": {},
   "source": [
    "Check out the config file. Mode should be set to `MEASURE` mode to perform calibration. And we'll use simple `maxabs` observer, which is the default. The result of calibration will be dumped under `calibration_outputs` directory. You can also specify which modules to quantize or not through `allowlist` and `blocklist`. In this walkthrough, we will specify nothing, which means we will quantize every module available."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6102b8d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "{\n",
    "    \"method\": \"HOOKS\",\n",
    "    \"mode\": \"MEASURE\",\n",
    "    \"observer\": \"maxabs\",\n",
    "    \"allowlist\": {\n",
    "        \"types\": [],\n",
    "        \"names\": []\n",
    "    },\n",
    "    \"blocklist\": {\n",
    "        \"types\": [],\n",
    "        \"names\": []\n",
    "    },\n",
    "    \"quantize_weight\": false,\n",
    "    \"dump_stats_path\": \"calibration_outputs/inc_output\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31224659",
   "metadata": {},
   "source": [
    "Initialize the vLLM engine with `quantization=\"inc\"`. This triggers the Intel Neural Compressor (INC) integration to hook into the model execution and collect statistics based on the `QUANT_CONFIG`.\n",
    "\n",
    "`PT_HPU_WEIGHT_SHARING=0` is required to free the full precision weights from the device and ensure only the FP8 weights are stored. And we'll disable the warm-up for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0bbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "os.environ[\"PT_HPU_WEIGHT_SHARING\"] = \"0\"\n",
    "os.environ[\"VLLM_SKIP_WARMUP\"] = \"true\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_PATH,\n",
    "    quantization=\"inc\",\n",
    "    max_model_len=2048,\n",
    "    distributed_executor_backend=\"mp\",\n",
    ")\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbde1ad",
   "metadata": {},
   "source": [
    "What happens under the hood? Below is a code snippet from `HPUModelRunner`. When the model is loaded by the runner, INC APIs are applied according to current mode. In calibration mode, `prepare` function configures the model by replacing target modules with their corresponding patched versions defined in INC. These patched modules include forward hooks that collect statistics during the model’s forward pass, which are then stored in a file for later use in the `QUANTIZE` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66df5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from neural_compressor.torch.quantization import (FP8Config, convert, prepare)\n",
    "\n",
    "config = FP8Config.from_json_file(os.getenv(\"QUANT_CONFIG\", \"\"))\n",
    "\n",
    "if config.measure:\n",
    "    self.model = prepare(self.model, config)\n",
    "elif config.quantize:\n",
    "    self.model = convert(self.model, config)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ead752",
   "metadata": {},
   "source": [
    "We need a calibration dataset. We'll use a subset of **Pile-10k** dataset and filter for samples that are long enough to provide meaningful activation statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_dataset_prompts(num_samples, least_tokens):\n",
    "    print(f\"Loading {num_samples} samples...\")\n",
    "    dataset = load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    samples = []\n",
    "    \n",
    "    for data in tqdm(dataset):\n",
    "        prompt = data[\"text\"]\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        if len(tokens.input_ids[0]) < least_tokens:\n",
    "            continue\n",
    "        samples.append(prompt)\n",
    "        if len(samples) >= num_samples:\n",
    "            break\n",
    "            \n",
    "    prompt_token_ids = []\n",
    "    for prompt in samples:\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=least_tokens)\n",
    "        prompt_token_ids.append(tokens.input_ids[0].tolist())\n",
    "        \n",
    "    return prompt_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0995f57",
   "metadata": {},
   "source": [
    "Now we run inference on the calibration dataset. The `llm.generate` call will pass data through the model, and the hooks (configured via `QUANT_CONFIG`) will record the statistics of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec93ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DATASET_SAMPLES = 128\n",
    "SAMPLE_LEN = 1024\n",
    "\n",
    "prompt_token_ids = get_dataset_prompts(\n",
    "    MAX_DATASET_SAMPLES, SAMPLE_LEN\n",
    ")\n",
    "input_batch = [{\"prompt_token_ids\": p} for p in prompt_token_ids]\n",
    "\n",
    "outputs = llm.generate(input_batch, sampling_params, use_tqdm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511fe6d0",
   "metadata": {},
   "source": [
    "After calibration is complete, we release the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ba50c",
   "metadata": {},
   "source": [
    "This process is required to save the statistics to the specified path, since the API for saving the results--`finalize_calibration`--is currently implemented in the runner’s destructor, as shown below. You can now see the calibration results under `calibration_outputs` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6be0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def shutdown_inc(self):\n",
    "    can_finalize_inc = self._is_quant_with_inc() and \\\n",
    "        (self.model.model is not None) and \\\n",
    "        self.inc_initialized_successfully and \\\n",
    "        not self._is_inc_finalized\n",
    "    if can_finalize_inc:\n",
    "        from neural_compressor.torch.quantization import (finalize_calibration)\n",
    "        finalize_calibration(self.model.model)\n",
    "        self._is_inc_finalized = True\n",
    "\n",
    "def __del__(self):\n",
    "    self.shutdown_inc()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
