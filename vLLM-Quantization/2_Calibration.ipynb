{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2a286b",
   "metadata": {},
   "source": [
    "# [A-2] Running Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d9d775",
   "metadata": {},
   "source": [
    "*Make sure to restart the Kernel before executing this notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d7fcd",
   "metadata": {},
   "source": [
    "In this notebook, we will perform **calibration** for quantization. \n",
    "\n",
    "Calibration is a crucial step for Post-Training Quantization (PTQ). It involves running a representative dataset through the model to collect statistics (ranges) of activations and weights. These statistics are then used to calculate scale factors for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2c60f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"/workspace/models/hub\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a81baca",
   "metadata": {},
   "source": [
    "## Intel Neural Compressor (INC)\n",
    "\n",
    "[INC](https://github.com/intel/neural-compressor) is an open-source library for model compression developed by Intel. It provides implementations of calibration and quantization methods that can be easily applied to existing models. As it is developed primarily by Intel, it is highly optimized for Intel hardware, including the Gaudi series.\n",
    "\n",
    "INC is closely integrated into `vllm-gaudi`, allowing calibration and quantization to be applied seamlessly through the vLLM APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712db1b",
   "metadata": {},
   "source": [
    "Now, let's start running calibration first. We should specify configurations in a config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd34fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"QUANT_CONFIG\"] = f\"{os.getcwd()}/configs/measure.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba068831",
   "metadata": {},
   "source": [
    "Check out the config file. Mode should be set to `MEASURE` mode to perform calibration. And we'll use simple `maxabs` observer, which is the default. The result of calibration will be dumped under `calibration_outputs` directory. You can also specify which modules to quantize or not through `allowlist` and `blocklist`. In this walkthrough, we will specify nothing, which means we will quantize every module available."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6102b8d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "{\n",
    "    \"method\": \"HOOKS\",\n",
    "    \"mode\": \"MEASURE\",\n",
    "    \"observer\": \"maxabs\",\n",
    "    \"allowlist\": {\n",
    "        \"types\": [],\n",
    "        \"names\": []\n",
    "    },\n",
    "    \"blocklist\": {\n",
    "        \"types\": [],\n",
    "        \"names\": []\n",
    "    },\n",
    "    \"quantize_weight\": false,\n",
    "    \"dump_stats_path\": \"calibration_outputs/inc_output\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31224659",
   "metadata": {},
   "source": [
    "Initialize the vLLM engine with `quantization=\"inc\"`. This triggers the Intel Neural Compressor (INC) integration to hook into the model execution and collect statistics based on the `QUANT_CONFIG`.\n",
    "\n",
    "`PT_HPU_WEIGHT_SHARING=0` is required to free the full precision weights from the device and ensure only the FP8 weights are stored. And we'll disable the warm-up for calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0bbd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-15 07:46:07 [__init__.py:40] Available plugins for group vllm.platform_plugins:\n",
      "INFO 12-15 07:46:07 [__init__.py:42] - hpu -> vllm_gaudi:register\n",
      "INFO 12-15 07:46:07 [__init__.py:45] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 12-15 07:46:07 [__init__.py:217] Platform plugin hpu is activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/habana_frameworks/torch/core/__init__.py:106: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  ret = original_fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-15 07:46:09 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 12-15 07:46:09 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 12-15 07:46:09 [interface.py:201] Failed to import from vllm._C: ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "WARNING 12-15 07:46:10 [nixl_connector.py:75] NIXL is not available\n",
      "WARNING 12-15 07:46:10 [nixl_connector.py:84] NIXL agent config is not available\n",
      "WARNING 12-15 07:46:10 [platform.py:141] Pin memory is not supported on HPU.\n",
      "WARNING 12-15 07:46:10 [registry.py:740] Model architecture Gemma3ForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.gemma3_mm:HpuGemma3ForConditionalGeneration.\n",
      "INFO 12-15 07:46:10 [utils.py:253] non-default args: {'max_model_len': 2048, 'distributed_executor_backend': 'mp', 'disable_log_stats': True, 'quantization': 'inc', 'model': 'Qwen/Qwen3-8B'}\n",
      "INFO 12-15 07:46:12 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parse safetensors files: 100%|██████████| 5/5 [00:00<00:00, 10.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-15 07:46:13 [model.py:1745] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2025-12-15 07:46:15,099\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-15 07:46:15 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 12-15 07:46:15 [platform.py:106] On HPU, VLLM_WORKER_MULTIPROC_METHOD=fork might cause application hangs on exit. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. To override that behavior, please set VLLM_WORKER_MULTIPROC_METHOD=fork explicitly.\n",
      "INFO 12-15 07:46:15 [platform.py:130] [HPU] Forcing CompilationMode.NONE compilation mode\n",
      "=========compilation_config.custom_ops=['all']===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-15 07:46:19 [__init__.py:40] Available plugins for group vllm.platform_plugins:\n",
      "INFO 12-15 07:46:19 [__init__.py:42] - hpu -> vllm_gaudi:register\n",
      "INFO 12-15 07:46:19 [__init__.py:45] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 12-15 07:46:19 [__init__.py:217] Platform plugin hpu is activated\n",
      "INFO 12-15 07:46:20 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 12-15 07:46:20 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 12-15 07:46:21 [interface.py:201] Failed to import from vllm._C: ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8628)\u001b[0;0m WARNING 12-15 07:46:21 [nixl_connector.py:75] NIXL is not available\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8628)\u001b[0;0m WARNING 12-15 07:46:21 [nixl_connector.py:84] NIXL agent config is not available\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8628)\u001b[0;0m WARNING 12-15 07:46:21 [platform.py:141] Pin memory is not supported on HPU.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8628)\u001b[0;0m WARNING 12-15 07:46:21 [registry.py:740] Model architecture Gemma3ForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.gemma3_mm:HpuGemma3ForConditionalGeneration.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8628)\u001b[0;0m INFO 12-15 07:46:21 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=inc, enforce_eager=False, kv_cache_dtype=auto, device_config=hpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'hpu_backend', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': None, 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8628)\u001b[0;0m WARNING 12-15 07:46:21 [multiproc_executor.py:869] Reducing Torch parallelism from 80 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-15 07:46:24 [__init__.py:40] Available plugins for group vllm.platform_plugins:\n",
      "INFO 12-15 07:46:24 [__init__.py:42] - hpu -> vllm_gaudi:register\n",
      "INFO 12-15 07:46:24 [__init__.py:45] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 12-15 07:46:24 [__init__.py:217] Platform plugin hpu is activated\n",
      "INFO 12-15 07:46:26 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
      "INFO 12-15 07:46:26 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "WARNING 12-15 07:46:26 [interface.py:201] Failed to import from vllm._C: ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
      "WARNING 12-15 07:46:26 [nixl_connector.py:75] NIXL is not available\n",
      "WARNING 12-15 07:46:26 [nixl_connector.py:84] NIXL agent config is not available\n",
      "WARNING 12-15 07:46:26 [platform.py:141] Pin memory is not supported on HPU.\n",
      "WARNING 12-15 07:46:26 [registry.py:740] Model architecture Gemma3ForConditionalGeneration is already registered, and will be overwritten by the new model class vllm_gaudi.models.gemma3_mm:HpuGemma3ForConditionalGeneration.\n",
      "INFO 12-15 07:46:26 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:57017 backend=hccl\n",
      "INFO 12-15 07:46:26 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "============================= HPU PT BRIDGE CONFIGURATION ON RANK = 0 ============= \n",
      " PT_HPU_LAZY_MODE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = ,false,1024,false\n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_EAGER_PIPELINE_ENABLE = 1\n",
      " PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n",
      " PT_HPU_ENABLE_LAZY_COLLECTIVES = 0\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 2015 GB\n",
      "------------------------------------------------------------------------------\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-15 07:46:29 [runtime.py:28] Environment:\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     hw: gaudi2\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     build: 1.22.2.32\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     engine_version: v1\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     bridge_mode: eager\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     model_type: qwen3\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     prefix_caching: True\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     vllm_gaudi_commit: Error getting commit hash\n",
      "INFO 12-15 07:46:29 [runtime.py:28] Features:\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     fp32_alibi_biases: True\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     fp32_softmax: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     fused_block_softmax_adjustment: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     fused_block_softmax: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     prompt_attn_impl: fsdpa_impl\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     skip_warmup: True\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     merged_prefill: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     use_contiguous_pa: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     use_bucketing: True\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     bucketing_strategy: exponential_bucketing\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     defrag: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     regional_compilation: True\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     dynamic_shapes_compilation: True\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     fullgraph_compilation: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     unified_attn: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     scale_adjustment: True\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     flatten_input: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     unified_attn_shared_cache_ratio: 1.0\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     high_level_profiler_enabled: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     track_graph_compilation: False\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     use_output_tensor_in_matmulqk: False\n",
      "INFO 12-15 07:46:29 [runtime.py:28] User flags:\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     PT_HPU_WEIGHT_SHARING: 0\n",
      "INFO 12-15 07:46:29 [runtime.py:32]     RUNTIME_SCALE_PATCHING: 1\n",
      "INFO 12-15 07:46:29 [platform.py:66] Using HPUAttentionV1 backend.\n",
      "INFO 12-15 07:46:29 [hpu_model_runner.py:970] Bucketing is ON.\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m \n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m         [TO BE DEPRECATED] Please use hpu_inference_set_env instead\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m         \n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:31 [hpu_model_runner.py:3572] Starting to load model Qwen/Qwen3-8B...\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:31 [platform.py:66] Using HPUAttentionV1 backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:00<00:01,  2.00it/s]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:01<00:03,  1.01s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:03<00:02,  1.34s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:05<00:01,  1.49s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:07<00:00,  1.70s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:07<00:00,  1.48s/it]\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:40 [default_loader.py:314] Loading weights took 7.45 seconds\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:40 [hpu_model_runner.py:3578] Loading model weights took 0.0000 GB\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:40 [hpu_model_runner.py:3581] Preparing model with INC..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m 2025-12-15 07:46:41 [WARNING][auto_accelerator.py:463] Auto detect accelerator: HPU_Accelerator.\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m 2025-12-15 07:46:41 [WARNING][auto_accelerator.py:463] Auto detect accelerator: HPU_Accelerator.\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m 2025-12-15 07:46:41 [INFO][hpu_model_runner.py:3588] Preparation started.\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m 2025-12-15 07:46:41 [INFO][quantize.py:175] Start to prepare model with fp8_quant.\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m 2025-12-15 07:46:48 [INFO][hpu_model_runner.py:3588] Preparation end.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m \n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m         [TO BE DEPRECATED] Please use hpu_inference_initialize instead\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m         \n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING: The argument 'mark_only_scales_as_const' will be removed soon. Please use 'mark_scales' instead.\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m         If mark_only_scales_as_const=True or mark_scales=True, then only scales are marked as const.\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m         If mark_non_scales=True, then non scale tensors are marked as constants.\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m         By default mark_only_scales_as_const=False, mark_scales=True, mark_non_scales=True\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m         \n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:48 [hpu_model_runner.py:3598] Preparing model with INC took 15.2662 GB\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:48 [hpu_model_runner.py:3617] Wrapping in HPUGraph took 0.0000 GB\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:49 [hpu_model_runner.py:3645] Compilation took 0.0000 GB\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:49 [hpu_worker.py:200] Model profiling run took 0 B of device memory (15.27 GiB/94.62 GiB used) and -256 KiB of host memory (123.9 GiB/1.968 TiB used)\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:49 [hpu_worker.py:224] Free device memory: 79.35 GiB, 71.42 GiB usable (gpu_memory_utilization=0.9), 7.142 GiB reserved for HPUGraphs (VLLM_GRAPH_RESERVED_MEM=0.1), 18 MiB reserved for KV cache dummy block 64.26 GiB reserved for usable KV cache\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8628)\u001b[0;0m INFO 12-15 07:46:49 [kv_cache_utils.py:1229] GPU KV cache size: 467,840 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8628)\u001b[0;0m INFO 12-15 07:46:49 [kv_cache_utils.py:1234] Maximum concurrency for 2,048 tokens per request: 228.44x\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [hpu_worker.py:243] Usable num_blocks: 3655, actual allocated num_blocks: 467968 (_PAD_BLOCK_ID=3655, _PAD_SLOT_ID=467840)\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [hpu_worker.py:246] Initializing cache engine took 64.27 GiB of device memory (79.54 GiB/94.62 GiB used) and 304 KiB of host memory (123.8 GiB/1.968 TiB used)\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [exponential.py:65] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 1, 1], query:[128, 128, 8192, 13], blocks:[0, 1, 15, 5]\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [common.py:204] Generated 36 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 128, 1), (1, 128, 2), (1, 128, 4), (1, 128, 8), (1, 128, 15), (1, 256, 0), (1, 256, 1), (1, 256, 2), (1, 256, 4), (1, 256, 8), (1, 384, 0), (1, 384, 1), (1, 384, 2), (1, 384, 4), (1, 384, 8), (1, 512, 0), (1, 512, 1), (1, 512, 2), (1, 512, 4), (1, 512, 8), (1, 768, 0), (1, 768, 1), (1, 768, 2), (1, 768, 4), (1, 768, 8), (1, 1024, 0), (1, 1024, 1), (1, 1024, 2), (1, 1024, 4), (1, 1024, 8), (1, 1536, 0), (1, 1536, 1), (1, 1536, 2), (1, 1536, 4), (1, 2048, 0)]\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [exponential.py:86] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[1, 256, 3655, 13]\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [common.py:204] Generated 18 decode buckets [bs, query, num_blocks]: [(2, 1, 32), (4, 1, 64), (8, 1, 128), (16, 1, 256), (32, 1, 256), (32, 1, 512), (64, 1, 256), (64, 1, 512), (64, 1, 1024), (128, 1, 256), (128, 1, 512), (128, 1, 1024), (128, 1, 2048), (256, 1, 256), (256, 1, 512), (256, 1, 1024), (256, 1, 2048), (256, 1, 3840)]\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [hpu_model_runner.py:4382] Skipping warmup...\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [exponential.py:65] Prompt bucket config (min, step, max_warmup, limit) bs:[1, 2, 1, 1], query:[128, 128, 8192, 13], blocks:[0, 1, 15, 5]\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [common.py:204] Generated 36 prompt buckets [bs, query, num_blocks]: [(1, 128, 0), (1, 128, 1), (1, 128, 2), (1, 128, 4), (1, 128, 8), (1, 128, 15), (1, 256, 0), (1, 256, 1), (1, 256, 2), (1, 256, 4), (1, 256, 8), (1, 384, 0), (1, 384, 1), (1, 384, 2), (1, 384, 4), (1, 384, 8), (1, 512, 0), (1, 512, 1), (1, 512, 2), (1, 512, 4), (1, 512, 8), (1, 768, 0), (1, 768, 1), (1, 768, 2), (1, 768, 4), (1, 768, 8), (1, 1024, 0), (1, 1024, 1), (1, 1024, 2), (1, 1024, 4), (1, 1024, 8), (1, 1536, 0), (1, 1536, 1), (1, 1536, 2), (1, 1536, 4), (1, 2048, 0)]\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [exponential.py:86] Decode bucket config (min, step, max_warmup, limit) bs:[1, 2, 256, 9], block:[1, 256, 3655, 13]\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [common.py:204] Generated 18 decode buckets [bs, query, num_blocks]: [(2, 1, 32), (4, 1, 64), (8, 1, 128), (16, 1, 256), (32, 1, 256), (32, 1, 512), (64, 1, 256), (64, 1, 512), (64, 1, 1024), (128, 1, 256), (128, 1, 512), (128, 1, 1024), (128, 1, 2048), (256, 1, 256), (256, 1, 512), (256, 1, 1024), (256, 1, 2048), (256, 1, 3840)]\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:46:50 [hpu_model_runner.py:4382] Skipping warmup...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8628)\u001b[0;0m INFO 12-15 07:46:50 [core.py:250] init engine (profile, create kv cache, warmup model) took 0.95 seconds\n",
      "INFO 12-15 07:46:51 [llm.py:352] Supported tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "os.environ[\"PT_HPU_WEIGHT_SHARING\"] = \"0\"\n",
    "os.environ[\"VLLM_SKIP_WARMUP\"] = \"true\"\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen3-8B\",\n",
    "    quantization=\"inc\",\n",
    "    max_model_len=2048,\n",
    "    distributed_executor_backend=\"mp\",\n",
    ")\n",
    "sampling_params = SamplingParams(temperature=0.0, max_tokens=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbde1ad",
   "metadata": {},
   "source": [
    "What happens under the hood? Below is a code snippet from `HPUModelRunner`. When the model is loaded by the runner, INC APIs are applied according to current mode. In calibration mode, `prepare` function configures the model by replacing target modules with their corresponding patched versions defined in INC. These patched modules include forward hooks that collect statistics during the model’s forward pass, which are then stored in a file for later use in the `QUANTIZE` step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66df5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom neural_compressor.torch.quantization import (FP8Config, convert, prepare)\\n\\nconfig = FP8Config.from_json_file(os.getenv(\"QUANT_CONFIG\", \"\"))\\n\\nif config.measure:\\n    self.model = prepare(self.model, config)\\nelif config.quantize:\\n    self.model = convert(self.model, config)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from neural_compressor.torch.quantization import (FP8Config, convert, prepare)\n",
    "\n",
    "config = FP8Config.from_json_file(os.getenv(\"QUANT_CONFIG\", \"\"))\n",
    "\n",
    "if config.measure:\n",
    "    self.model = prepare(self.model, config)\n",
    "elif config.quantize:\n",
    "    self.model = convert(self.model, config)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ead752",
   "metadata": {},
   "source": [
    "We need a calibration dataset. We'll use a subset of **Pile-10k** dataset and filter for samples that are long enough to provide meaningful activation statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91f3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_dataset_prompts(num_samples, least_tokens):\n",
    "    print(f\"Loading {num_samples} samples...\")\n",
    "    dataset = load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B\", trust_remote_code=True)\n",
    "    samples = []\n",
    "    \n",
    "    for data in tqdm(dataset):\n",
    "        prompt = data[\"text\"]\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        if len(tokens.input_ids[0]) < least_tokens:\n",
    "            continue\n",
    "        samples.append(prompt)\n",
    "        if len(samples) >= num_samples:\n",
    "            break\n",
    "            \n",
    "    prompt_token_ids = []\n",
    "    for prompt in samples:\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=least_tokens)\n",
    "        prompt_token_ids.append(tokens.input_ids[0].tolist())\n",
    "        \n",
    "    return prompt_token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0995f57",
   "metadata": {},
   "source": [
    "Now we run inference on the calibration dataset. The `llm.generate` call will pass data through the model, and the hooks (configured via `QUANT_CONFIG`) will record the statistics of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec93ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 128 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 10000/10000 [00:00<00:00, 39021.14 examples/s]\n",
      "  6%|▌         | 610/10000 [00:01<00:22, 417.86it/s]\n",
      "Adding requests: 100%|██████████| 128/128 [00:00<00:00, 7779.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:00 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 1024, 0) was not warmed-up!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:04 [hpu_model_runner.py:2538] Configuration: ('decode', 8, 1, 128) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:06 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 128, 8) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:10 [hpu_model_runner.py:2538] Configuration: ('decode', 16, 1, 256) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:14 [hpu_model_runner.py:2538] Configuration: ('decode', 32, 1, 256) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:15 [hpu_model_runner.py:2538] Configuration: ('decode', 32, 1, 512) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:16 [hpu_model_runner.py:2538] Configuration: ('decode', 64, 1, 512) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:18 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 256, 8) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:20 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 768, 0) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:21 [hpu_model_runner.py:2538] Configuration: ('decode', 64, 1, 1024) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:22 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 384, 8) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:23 [hpu_model_runner.py:2538] Configuration: ('decode', 128, 1, 1024) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:25 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 512, 8) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:26 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 512, 0) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:27 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 768, 4) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:28 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 384, 0) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:29 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 256, 0) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:29 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 1024, 2) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:30 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 128, 0) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:30 [hpu_model_runner.py:2538] Configuration: ('decode', 128, 1, 2048) was not warmed-up!\n",
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:31 [hpu_model_runner.py:2538] Configuration: ('prompt', 1, 1024, 1) was not warmed-up!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  94%|█████████▍| 120/128 [00:38<00:00, 48.33it/s, est. speed input: 3214.48 toks/s, output: 100.45 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m WARNING 12-15 07:47:38 [hpu_model_runner.py:2538] Configuration: ('decode', 2, 1, 32) was not warmed-up!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 128/128 [00:39<00:00,  3.27it/s, est. speed input: 3351.24 toks/s, output: 104.73 toks/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_DATASET_SAMPLES = 128\n",
    "SAMPLE_LEN = 1024\n",
    "\n",
    "prompt_token_ids = get_dataset_prompts(\n",
    "    MAX_DATASET_SAMPLES, SAMPLE_LEN\n",
    ")\n",
    "input_batch = [{\"prompt_token_ids\": p} for p in prompt_token_ids]\n",
    "\n",
    "outputs = llm.generate(input_batch, sampling_params, use_tqdm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511fe6d0",
   "metadata": {},
   "source": [
    "After calibration is complete, we release the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b34000a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker pid=8790)\u001b[0;0m INFO 12-15 07:47:39 [multiproc_executor.py:702] Parent process exited, terminating worker\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8628)\u001b[0;0m =========compilation_config.custom_ops=['all']===========\n"
     ]
    }
   ],
   "source": [
    "del llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ba50c",
   "metadata": {},
   "source": [
    "This process is required to save the statistics to the specified path, since the API for saving the results--`finalize_calibration`--is currently implemented in the runner’s destructor, as shown below. You can now see the calibration results under `calibration_outputs` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a6be0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef shutdown_inc(self):\\n    can_finalize_inc = self._is_quant_with_inc() and         (self.model.model is not None) and         self.inc_initialized_successfully and         not self._is_inc_finalized\\n    if can_finalize_inc:\\n        from neural_compressor.torch.quantization import (finalize_calibration)\\n        finalize_calibration(self.model.model)\\n        self._is_inc_finalized = True\\n\\ndef __del__(self):\\n    self.shutdown_inc()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def shutdown_inc(self):\n",
    "    can_finalize_inc = self._is_quant_with_inc() and \\\n",
    "        (self.model.model is not None) and \\\n",
    "        self.inc_initialized_successfully and \\\n",
    "        not self._is_inc_finalized\n",
    "    if can_finalize_inc:\n",
    "        from neural_compressor.torch.quantization import (finalize_calibration)\n",
    "        finalize_calibration(self.model.model)\n",
    "        self._is_inc_finalized = True\n",
    "\n",
    "def __del__(self):\n",
    "    self.shutdown_inc()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
