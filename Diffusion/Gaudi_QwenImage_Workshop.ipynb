{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [B] Optimizing Qwen-Image (Text-to-Image) Inference Hands-on with Intel® Gaudi®\n",
    "\n",
    "## What you will do\n",
    "- **Stage 0 (Fail on purpose)**: Eager mode(PT_HPU_LAZY_MODE=0) (no patch) → observe RoPE complex dtype error on Gaudi\n",
    "- **Stage 1 (Fix correctness)**: Eager mode(PT_HPU_LAZY_MODE=0) + Fix RoPE layer\n",
    "- **Stage 2 (Kernel Optimization)**: Eager mode(PT_HPU_LAZY_MODE=0) + Fix RoPE layer + Apply FusedSDPA(for Gaudi) kernel\n",
    "- **Stage 3 (Hpu graphs mode)**: HPU graph mode(PT_HPU_LAZY_MODE=1) + Fix RoPE layer + Apply FusedSDPA(for Gaudi) kernel\n",
    "\n",
    "\n",
    "## Stage runner\n",
    "We use `run_qwen_latency.py` as the single runner and toggle behavior using flags:\n",
    "- `--patch-rope`: apply Gaudi-optimized RoPE functions from `gaudi_transformer_qwenimage.py`\n",
    "- `--patch-fused-attn`: switch attention compute to the Habana fused kernel path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install diffusers==0.35.1\n",
    "!pip install transformers==4.55.4\n",
    "!pip install matplotlib\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Stage 0 — Unpatched run (expected to fail on Gaudi)\n",
    "We intentionally run **without** `--patch-rope` to show the RoPE failure on Gaudi:\n",
    "- Expected error: `RuntimeError: Complex datatype is not supported on Gaudi device.`\n",
    "\n",
    "This cell is written to **not stop the notebook** (it will catch the non-zero exit).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Strict mode: -u (error on undefined vars), -o pipefail (fail if any pipe fails)\n",
    "set -uo pipefail\n",
    "\n",
    "export PT_HPU_LAZY_MODE=0\n",
    "\n",
    "# Allow this command to fail without stopping the notebook\n",
    "# Stage 0 is expected to fail (RoPE complex dtype error)\n",
    "set +e\n",
    "python run_qwen_latency.py \\\n",
    "  --prompt \"A cute cat holding a sign that says hello world\" \\\n",
    "  --negative-prompt \" \" \\\n",
    "  --width 1024 --height 1024 \\\n",
    "  --steps 28 \\\n",
    "  --seed 42 \\\n",
    "  --num-runs 2 --skip 0 \\\n",
    "  --output stage0_unpatched.png\n",
    "rc=$?\n",
    "set -e\n",
    "\n",
    "# Force successful exit so subsequent cells can run\n",
    "exit 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 — Apply RoPE patch (make it work on Gaudi)\n",
    "Now we enable `--patch-rope` so the pipeline runs successfully on Gaudi.\n",
    "\n",
    "This stage keeps `PT_HPU_LAZY_MODE=0` so we isolate the **correctness fix** first.\n",
    "\n",
    "\n",
    "### Problem: Complex Data Types Not Supported on Gaudi\n",
    "\n",
    "The RoPE implementation in the Qwen-Image model uses **complex number operations**:\n",
    "\n",
    "```python\n",
    "# Original diffusers code (transformer_qwenimage.py)\n",
    "self.pos_freqs = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "```\n",
    "\n",
    "Attempting to move this complex tensor to an Gaudi device results in an error:\n",
    "\n",
    "```\n",
    "RuntimeError: Complex datatype is not supported on HPU device.\n",
    "\n",
    "```\n",
    "\n",
    "### Solution: Real Number-Based RoPE Implementation\n",
    "\n",
    "The `--patch-rope` flag replaces the original code with an optimized implementation in `gaudi_transformer_qwenimage.py`:\n",
    "\n",
    "| Aspect | Original (diffusers) | After Patch (Gaudi) |\n",
    "| --- | --- | --- |\n",
    "| Data Type | `complex64` | `bfloat16` (separate cos, sin) |\n",
    "| Operation Method | Uses `torch.polar()` | Calculates cos/sin separately |\n",
    "| Gaudi Compatibility | ❌ Not Supported | ✅ Fully Supported |\n",
    "\n",
    "### Code Comparison**Original (Using Complex Numbers):**\n",
    "\n",
    "```python\n",
    "# Uses complex polar form\n",
    "freqs = torch.polar(torch.ones_like(freqs), freqs)  # e^(i*theta)\n",
    "x_out = x * freqs  # Complex multiplication\n",
    "\n",
    "```\n",
    "\n",
    "**After Patch (Separated Real Numbers):**\n",
    "\n",
    "```python\n",
    "# Calculates cos and sin separately\n",
    "cos_freqs = torch.cos(freqs)\n",
    "sin_freqs = torch.sin(freqs)\n",
    "\n",
    "# Applies rotation using real number arithmetic\n",
    "out_real = x_real * cos - x_imag * sin\n",
    "out_imag = x_real * sin + x_imag * cos\n",
    "\n",
    "```\n",
    "\n",
    "### Mathematical EquivalenceComplex \n",
    "\n",
    "Rotation: $z \\cdot e^{i\\theta} = (a + bi)(cos\\theta + i \\cdot sin\\theta)$\n",
    "\n",
    "Separated Real Implementation:\n",
    "\n",
    "- $real = a \\cdot cos\\theta - b \\cdot sin\\theta$\n",
    "- $imag = a \\cdot sin\\theta + b \\cdot cos\\theta$\n",
    "\n",
    "Both methods produce **mathematically identical results**, but the separated real number method is executable on the HPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "export PT_Gaudi_LAZY_MODE=0\n",
    "\n",
    "python run_qwen_latency.py \\\n",
    "  --prompt \"A cute cat holding a sign that says hello world\" \\\n",
    "  --negative-prompt \" \" \\\n",
    "  --width 1024 --height 1024 \\\n",
    "  --steps 28 \\\n",
    "  --seed 42 \\\n",
    "  --num-runs 4 --skip 0 \\\n",
    "  --patch-rope \\\n",
    "  --output stage1_rope_eager.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 — RoPE + fused attention\n",
    "If your environment supports Habana fused kernels, this can further improve attention performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: Memory Bottleneck in Attention OperationsSelf-Attention in Transformers consists of the following steps:\n",
    "\n",
    "1. **QKV Projection**: Linear operation\n",
    "2. **Attention Score Calculation**: $QK^T / \\sqrt{d}$\n",
    "3. **Softmax**: Probability distribution for each position\n",
    "4. **Value Weighted Sum**: $\\text{softmax}(QK^T)V$\n",
    "\n",
    "In standard PyTorch implementations, **intermediate results are stored in memory** at each step.\n",
    "In particular, the Attention Score matrix consumes memory proportional to the square of the sequence length.\n",
    "\n",
    "### Solution: Habana FusedSDPA\n",
    "\n",
    "The `--patch-fused-attn` flag utilizes Habana's **FusedSDPA (Fused Scaled Dot-Product Attention)** kernel:\n",
    "\n",
    "```python\n",
    "from habana_frameworks.torch.hpex.kernels import FusedSDPA\n",
    "\n",
    "# Original: PyTorch SDPA (Stores intermediate results in memory)\n",
    "output = F.scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "# After Patch: Habana FusedSDPA (Fused processing within the kernel)\n",
    "# Note: Ensure arguments match the official API signature (q, k, v, mask, dropout, causal, scale)\n",
    "output = FusedSDPA.apply(query, key, value, None, 0.0, False, None, \"fast\", None)\n",
    "\n",
    "```\n",
    "\n",
    "### Performance Improvement Principles\n",
    "\n",
    "| Item | Standard SDPA | Habana FusedSDPA |\n",
    "| --- | --- | --- |\n",
    "| **Memory Access** | Read/Write at each step | Utilizes internal kernel registers |\n",
    "| **Intermediate Tensor Storage** | ✅ Stores Attention Scores | ❌ Does not store |\n",
    "| **Operation Fusion** | Individual kernel calls | Single fused kernel |\n",
    "| **Memory Bandwidth** | High usage | Optimized usage |\n",
    "\n",
    "### Qwen-Image Special Structure: Double-Stream Attention\n",
    "\n",
    "Qwen-Image uses a specialized structure that processes **text streams** and **image streams** together:\n",
    "\n",
    "```python\n",
    "# Combine QKV for text and image (Joint Attention)\n",
    "joint_query = torch.cat([txt_query, img_query], dim=1)\n",
    "joint_key = torch.cat([txt_key, img_key], dim=1)\n",
    "joint_value = torch.cat([txt_value, img_value], dim=1)\n",
    "\n",
    "# Integrated processing with FusedSDPA\n",
    "joint_hidden_states = FusedSDPA.apply(\n",
    "    joint_query.transpose(1, 2),\n",
    "    joint_key.transpose(1, 2),\n",
    "    joint_value.transpose(1, 2),\n",
    "    None, 0.0, False, None, \"fast\", None\n",
    ")\n",
    "\n",
    "# Separate results\n",
    "txt_output = joint_hidden_states[:, :seq_txt, :]\n",
    "img_output = joint_hidden_states[:, seq_txt:, :]\n",
    "\n",
    "```\n",
    "\n",
    "### Expected Benefits\n",
    "* **Reduced Latency**: Approx. 1.5x speedup\n",
    "* **Memory Efficiency**: Saves memory by not storing intermediate Attention Score tensors\n",
    "* **Increased Throughput**: Enables processing of larger batches or higher resolutions within the same memory footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "export PT_HPU_LAZY_MODE=0\n",
    "# Prevent OOM by setting large compound op size (diffusers sets this to 1 by default)\n",
    "export PT_HPU_MAX_COMPOUND_OP_SIZE=9223372036854775807\n",
    "\n",
    "# Stage 2: RoPE + fused attention\n",
    "python run_qwen_latency.py \\\n",
    "  --prompt \"A cute cat holding a sign that says hello world\" \\\n",
    "  --negative-prompt \" \" \\\n",
    "  --width 1024 --height 1024 \\\n",
    "  --steps 28 \\\n",
    "  --seed 42 \\\n",
    "  --num-runs 4 --skip 0 \\\n",
    "  --patch-rope \\\n",
    "  --patch-fused-attn \\\n",
    "  --output stage2_rope_sdpa_eager.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3 — Using HPU graph mode (after RoPE+FusedSDPA patch)\n",
    "Now that correctness is fixed, we switch to `PT_HPU_LAZY_MODE=1` + `wrap_in_hpu_graph` and measure latency again.\n",
    "\n",
    "### How `wrap_in_hpu_graph` is Applied\n",
    "\n",
    "When you run with `--mode hpu_graphs`, the runner script (`run_qwen_latency.py`) does the following:\n",
    "\n",
    "1. **Sets pipeline stages**: `pipe.stages = [\"transformer\"]`\n",
    "2. **Defines repeated blocks**: `pipe._repeated_blocks = [\"QwenImageTransformerBlock\"]`\n",
    "3. **Wraps individual blocks**: Instead of wrapping the entire transformer at once, `wrap_in_hpu_graph()` is applied to each `QwenImageTransformerBlock` separately\n",
    "\n",
    "```python\n",
    "# From run_qwen_latency.py: _compile_pipeline_modules()\n",
    "for name, module in transformer.named_modules():\n",
    "    if module.__class__.__name__ == \"QwenImageTransformerBlock\":\n",
    "        # Wrap each block individually\n",
    "        setattr(parent, child_name, wrap_in_hpu_graph(module))\n",
    "```\n",
    "\n",
    "**Why wrap blocks individually?**\n",
    "- **Shape flexibility**: Each block can have its own graph cache, avoiding locking to warmup shapes\n",
    "- **Memory efficiency**: Smaller graphs consume less memory during compilation\n",
    "- **Better performance**: Block-level graphs can be reused more efficiently across different sequence lengths\n",
    "\n",
    "**Required environment**:\n",
    "- `PT_HPU_LAZY_MODE=1`: Enables lazy execution (required for HPU graphs)\n",
    "- `PT_HPU_RECIPE_CACHE_CONFIG`: Specifies where to cache compiled graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "\n",
    "# Cache path to save compiled graphs recipe.\n",
    "export PT_HPU_RECIPE_CACHE_CONFIG=/tmp/gaudi_graphs,false,1024,false\n",
    "export PT_HPU_LAZY_MODE=1\n",
    "# Prevent OOM by setting large compound op size (diffusers sets this to 1 by default)\n",
    "export PT_HPU_MAX_COMPOUND_OP_SIZE=9223372036854775807\n",
    "\n",
    "python run_qwen_latency.py \\\n",
    "  --prompt \"A cute cat holding a sign that says hello world\" \\\n",
    "  --negative-prompt \" \" \\\n",
    "  --width 1024 --height 1024 \\\n",
    "  --steps 28 \\\n",
    "  --seed 42 \\\n",
    "  --num-runs 6 --skip 2 \\\n",
    "  --patch-rope \\\n",
    "  --patch-fused-attn \\\n",
    "  --mode hpu_graphs \\\n",
    "  --output stage3_rope_sdpa_graph.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency Comparison & Visualization\n",
    "\n",
    "Now that we've run all stages, let's compare the latency results:\n",
    "- **Stage 1**: RoPE patch only (eager mode) — correctness fix\n",
    "- **Stage 2**: RoPE + FusedSDPA (eager mode) — kernel optimization  \n",
    "- **Stage 3**: RoPE + FusedSDPA (HPU graphs + lazy mode) — graph compilation\n",
    "\n",
    "The following cells will:\n",
    "1. Load latency data from saved JSON files\n",
    "2. Display a comparison table with mean/p50/p90 latencies\n",
    "3. Calculate speedup ratios between stages\n",
    "4. Visualize results with bar charts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the stage files (matching output names from previous stages)\n",
    "stage_files = {\n",
    "    \"Stage 1\\n(RoPE only, eager)\": \"outputs/stage1_rope_eager.json\",\n",
    "    \"Stage 2\\n(RoPE + FusedAttn, eager)\": \"outputs/stage2_rope_sdpa_eager.json\",\n",
    "    \"Stage 3\\n(RoPE + FusedAttn, graphs)\": \"outputs/stage3_rope_sdpa_graph.json\",\n",
    "}\n",
    "\n",
    "# Load latency data from JSON files\n",
    "latency_results = {}\n",
    "for label, json_file in stage_files.items():\n",
    "    json_path = Path(json_file)\n",
    "    if json_path.exists():\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            latency_results[label] = {\n",
    "                \"mean\": data[\"statistics\"][\"mean_sec\"],\n",
    "                \"p50\": data[\"statistics\"][\"p50_sec\"],\n",
    "                \"p90\": data[\"statistics\"][\"p90_sec\"],\n",
    "                \"runs\": data[\"statistics\"][\"num_runs\"],\n",
    "                \"meta\": data[\"run_meta\"],\n",
    "            }\n",
    "            print(f\"✓ Loaded {json_file}: mean={data['statistics']['mean_sec']:.2f}s\")\n",
    "    else:\n",
    "        print(f\"✗ Not found: {json_file}\")\n",
    "\n",
    "# Print comparison table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"LATENCY COMPARISON SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Stage':<40} {'Mean (s)':<12} {'P50 (s)':<12} {'P90 (s)':<12}\")\n",
    "print(\"-\" * 70)\n",
    "for label, data in latency_results.items():\n",
    "    clean_label = label.replace('\\n', ' ')\n",
    "    print(f\"{clean_label:<40} {data['mean']:<12.2f} {data['p50']:<12.2f} {data['p90']:<12.2f}\")\n",
    "\n",
    "# Calculate speedups\n",
    "if len(latency_results) >= 2:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SPEEDUP ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    labels = list(latency_results.keys())\n",
    "    \n",
    "    # Stage 1->2 speedup\n",
    "    if len(labels) >= 2:\n",
    "        stage1_mean = latency_results[labels[0]][\"mean\"]\n",
    "        stage2_mean = latency_results[labels[1]][\"mean\"]\n",
    "        speedup_1_to_2 = stage1_mean / stage2_mean\n",
    "        print(f\"Stage 1->2: {speedup_1_to_2:.2f}x speedup\")\n",
    "    \n",
    "    # Stage 2->3 speedup\n",
    "    if len(labels) >= 3:\n",
    "        stage2_mean = latency_results[labels[1]][\"mean\"]\n",
    "        stage3_mean = latency_results[labels[2]][\"mean\"]\n",
    "        speedup_2_to_3 = stage2_mean / stage3_mean\n",
    "        print(f\"Stage 2->3: {speedup_2_to_3:.2f}x speedup\")\n",
    "    \n",
    "    # Stage 1->3 speedup\n",
    "    if len(labels) >= 3:\n",
    "        stage1_mean = latency_results[labels[0]][\"mean\"]\n",
    "        stage3_mean = latency_results[labels[2]][\"mean\"]\n",
    "        speedup_1_to_3 = stage1_mean / stage3_mean\n",
    "        print(f\"Stage 1->3: {speedup_1_to_3:.2f}x speedup\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Graph Visualization\n",
    "if latency_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Color palette\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    # ===== Plot 1: Mean Latency Comparison =====\n",
    "    ax1 = axes[0]\n",
    "    labels = list(latency_results.keys())\n",
    "    means = [latency_results[l][\"mean\"] for l in labels]\n",
    "    \n",
    "    bars = ax1.bar(labels, means, color=colors[:len(labels)], edgecolor='white', linewidth=2)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean in zip(bars, means):\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{mean:.1f}s',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 5),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax1.set_ylabel('Latency (seconds)', fontsize=12)\n",
    "    ax1.set_title('Mean Latency per Stage', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylim(0, max(means) * 1.2)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    \n",
    "    # ===== Plot 2: Speedup vs Stage 1 =====\n",
    "    ax2 = axes[1]\n",
    "    if len(latency_results) >= 2:\n",
    "        base_mean = means[0]\n",
    "        speedups = [base_mean / m for m in means]\n",
    "        \n",
    "        bars2 = ax2.bar(labels, speedups, color=colors[:len(labels)], edgecolor='white', linewidth=2)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, speedup in zip(bars2, speedups):\n",
    "            height = bar.get_height()\n",
    "            ax2.annotate(f'{speedup:.2f}x',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 5),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom',\n",
    "                        fontsize=12, fontweight='bold')\n",
    "        \n",
    "        ax2.axhline(y=1, color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "        ax2.set_ylabel('Speedup (vs Stage 1)', fontsize=12)\n",
    "        ax2.set_title('Speedup Relative to Stage 1', fontsize=14, fontweight='bold')\n",
    "        ax2.set_ylim(0, max(speedups) * 1.3)\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        ax2.spines['top'].set_visible(False)\n",
    "        ax2.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/latency_comparison.png', dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(\"\\n[output] saved_chart=outputs/latency_comparison.png\")\n",
    "else:\n",
    "    print(\"No latency data available. Please run the stages first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generated images side by side\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "image_files = {\n",
    "    \"Stage 1 (RoPE, eager)\": \"outputs/stage1_rope_eager.png\",\n",
    "    \"Stage 2 (RoPE + SDPA, eager)\": \"outputs/stage2_rope_sdpa_eager.png\",\n",
    "    \"Stage 3 (RoPE + SDPA, hpu_graphs)\": \"outputs/stage3_rope_sdpa_graph.png\",\n",
    "}\n",
    "\n",
    "existing_images = {k: v for k, v in image_files.items() if os.path.exists(v)}\n",
    "\n",
    "if existing_images:\n",
    "    num_images = len(existing_images)\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(6 * num_images, 6))\n",
    "    \n",
    "    if num_images == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (label, img_path) in zip(axes, existing_images.items()):\n",
    "        img = Image.open(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(label, fontsize=12, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Add latency info if available\n",
    "        json_path = img_path.replace('.png', '.json')\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                latency = data['statistics']['mean_sec']\n",
    "                ax.set_xlabel(f\"Mean: {latency:.2f}s\", fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('outputs/image_comparison.png', dpi=100, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    print(\"[output] saved_chart=outputs/image_comparison.png\")\n",
    "else:\n",
    "    print(\"No images found. Please run the stages first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
