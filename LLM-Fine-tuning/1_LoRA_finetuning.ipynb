{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1d259d",
   "metadata": {},
   "source": [
    "# [C-1]. Fine-tuning LLM with Intel® Gaudi®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1356f1f",
   "metadata": {},
   "source": [
    "# What you will do\n",
    "\n",
    "In this tutorial, you will fine-tune the Qwen3-0.6B model for Causal Language Modeling using Optimum Habana. This guide builds upon examples from the [huggingface/optimum-habana](https://github.com/huggingface/optimum-habana) repository.\n",
    "\n",
    "We will focus on adapting the model to a specific \"Joseon Dynasty\" persona using the following techniques:\n",
    "- DeepSpeed: optimizing memory usage for distributed training on Gaudi.\n",
    "- LoRA (Low-Rank Adaptation): Efficient fine-tuning by freezing pre-trained weights.\n",
    "- GraLoRA (Granular LoRA): An advanced PEFT method for better expressiveness.\n",
    "\n",
    "**Note on Scalability**: While this workshop utilizes a single Gaudi device, the fine-tuning code provided is fully implemented and optimized for multi-Gaudi distributed training. The workflows you learn here are designed to scale seamlessly.\n",
    "\n",
    "## What is Causal Language Modeling?\n",
    "\n",
    "Causal language modeling is the task of predicting the token following a sequence of tokens. In this scenario, the model **only attends to the left context** (tokens previously generated or provided). This training objective is essential for generation tasks like chatbots and story completion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c75829",
   "metadata": {},
   "source": [
    "# 1. Install Dependencies\n",
    "\n",
    "First, we install the necessary libraries. This includes **Optimum Habana** for Gaudi support, **PEFT** for parameter-efficient fine-tuning, and **DeepSpeed** for distributed training optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51936f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Optimum Habana from main\n",
    "!pip install git+https://github.com/huggingface/optimum-habana@main \n",
    "\n",
    "# Install standard data processing and visualization libraries\n",
    "!pip install datasets sentencepiece protobuf scikit-learn pandas matplotlib\n",
    "\n",
    "# Install the latest PEFT library\n",
    "!pip install git+https://github.com/huggingface/peft@main \n",
    "\n",
    "# Install Habana-optimized DeepSpeed\n",
    "!pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.22.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d18d7",
   "metadata": {},
   "source": [
    "# 2. Dataset Prepartion\n",
    "\n",
    "## Dataset: Joseon Persona\n",
    "\n",
    "We are using a custom Korean fine-tuning dataset designed to train an LLM to respond to modern daily inquiries with the persona and tone of a loyal subject from the Joseon Dynasty.\n",
    "\n",
    "Examples:\n",
    "\n",
    "| Input | Output|\n",
    "| --- | --- |\n",
    "| 돈 아껴 쓰라고 해줘. | 티끌 모아 태산이라 했습니다. 그 푼돈을 아껴야 나중에 기와집이라도 한 채 사시지 않겠사옵니까. |\n",
    "| 겨울 간식 추천해줘. | 뜨끈한 어묵 국물과 함께 먹는 붕어빵, 그리고 귤이 겨울의 맛이옵니다. |\n",
    "| 운동하라고 닥달해줘. | 뱃살이 인덕이라 우길 단계는 지났사옵니다. 당장 일어나 뛰시옵소서! |\n",
    "\n",
    "You can view the full dataset [here](./joseon_persona_dataset.csv).\n",
    "Here, we use a specific alpaca-chat format that matches the Joseon Persona dataset structure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5ab9f",
   "metadata": {},
   "source": [
    "# 3. Training Configuration\n",
    "\n",
    "### Fine-tuning Qwen/Qwen3-0.6B on Intel Gaudi\n",
    "\n",
    "We will configure our training in three parts:\n",
    "1. PEFT Configuration: Selecting the method and its hyperparameters\n",
    "2. Training Arguments: Standart HuggingFace training arguments (learning rate, epochs, etc.).\n",
    "3. Gaudi Arguments: Gaudi-specific configurations\n",
    "\n",
    "## 3.1. PEFT Arguments\n",
    "\n",
    "We start by defining the LoRA parameters. LoRA reduces the number of trainable parameters by injecting rank-decomposition matrices into each layer of the Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b2b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define PEFT (LoRA) Parameters\n",
    "input_kwargs = {\n",
    "    \"peft_type\": \"lora\",\n",
    "    \"lora_rank\": 64,\n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc35d3",
   "metadata": {},
   "source": [
    "## 3.2. Training Arguments\n",
    "\n",
    "We then define the standart training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f430c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Standard Transformer Training Arguments\n",
    "transformer_kwargs = {\n",
    "    \"model_name_or_path\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"num_train_epochs\": 20,\n",
    "    \"seed\": 42,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"per_device_train_batch_size\": 256,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"logging_steps\": 1,\n",
    "}\n",
    "\n",
    "# Merge arguments\n",
    "input_kwargs.update(transformer_kwargs)\n",
    "\n",
    "# Generate a unique output directory name based on hyperparameters\n",
    "name_args = [\"peft_type\", \"lora_rank\", \"lora_alpha\", \"num_train_epochs\", \"learning_rate\"]\n",
    "lora_output_dir = f\"./finetuned_models/joseon_persona_qwen3_0.6b\"\n",
    "for arg in name_args:\n",
    "    lora_output_dir += f\"_{arg.replace('lora_', '')}-{input_kwargs[arg]}\"\n",
    "    \n",
    "print(f\"Model will be saved in: {lora_output_dir}\")\n",
    "input_kwargs[\"output_dir\"] = lora_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a61151c",
   "metadata": {},
   "source": [
    "## 3.3. Gaudi Specific Argtuments\n",
    "\n",
    "To enable training on Intel Gaudi, we must replace the standard HuggingFace Transformer classes with their Optimum-Habana counterparts. \n",
    "Specifically, `Trainer` is replaced by `GaudiTrainer`, and `TrainingArguments` is replaced by `GaudiTrainingArguments`.\n",
    "\n",
    "### Why `GaudiTrainer`\n",
    "The `GaudiTrainer` is a wrapper built around the standard Transformer `Trainer`. Its primary role is to integrate the `gaudi_config` argument, which orchestrates essential hardware-specific behaviors:\n",
    "- Mixed Precision: Utilizing BF16/FP32 autocasting.\n",
    "- Fused Operations: Leveraging Habana's custom AdamW or Clip Norm implementations.\n",
    "\n",
    "Additional information and exact implementation of `GaudiTrainer` can be found [here](https://github.com/huggingface/optimum-habana/blob/main/optimum/habana/transformers/trainer.py#L214)\n",
    "\n",
    "### Why `GaudiTrainingArguments`\n",
    "The `GaudiTrainingArguments` class extends standard Transformer `TrainingArguments` to include parameters critical for managing Gaudi execution flow: \n",
    "- Training Mode (Lazy vs. Eager): Determines the execution backend. It is typically used to enable Lazy Mode, which accumulates operations to build and compile efficient computation graphs rather than executing them eagerly operation-by-operation.\n",
    "- Compilation & Cache Limits: Controls the Gaudi graph compiler's behavior, allowing you to set limits on cache size or the maximum number of compiled graphs to prevent out-of-memory (OOM) errors during dynamic shape variations.\n",
    "- Profiling & Throughput: Provides specific arguments to fine-tune performance measurement, such as defining warm-up steps and identifying the exact number of steps to capture for accurate throughput calculation.\n",
    "\n",
    "Additional information and exact implementation of `GaudiTrainerArguments` can be found [here](https://github.com/huggingface/optimum-habana/blob/main/optimum/habana/transformers/training_args.py#L86)\n",
    "\n",
    "### Define Gaudi Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc3768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Gaudi Specific Arguments\n",
    "gaudi_kwargs = {\n",
    "    \"use_habana\": True,  # Whether to use Gaudi or not\n",
    "    \"use_lazy_mode\": True,  # Whether to use lazy or eager mode\n",
    "    \"gaudi_config_name\": \"Habana/qwen\",  # Gaudi configuration to use\n",
    "}\n",
    "input_kwargs.update(gaudi_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a44b9",
   "metadata": {},
   "source": [
    "# 4. Distributed Training with DeepSpeed\n",
    "\n",
    "To efficiently manage memory and train larger models, we leverage **DeepSpeed**. DeepSpeed allows us to partition optimizer states and gradients across processes.\n",
    "\n",
    "We will use the `DistributedRunner` class to launch the training script. This utility handles the complexity of spawning multiple processes on the Gaudi.\n",
    "\n",
    "Steps:\n",
    "1. Define the DeepSpeed configuration.\n",
    "2. Instantiate DistributedRunner with the command to run.\n",
    "\n",
    "### Launch Training\n",
    "- The training log will be saved in the `./logs` directory.\n",
    "- The loss curve will be saved in the `./train_loss` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32249fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from optimum.habana.distributed import DistributedRunner\n",
    "\n",
    "# Add DeepSpeed config to arguments\n",
    "input_kwargs[\"deepspeed\"] = \"configs/deepspeed_zero_1.json\" \n",
    "\n",
    "# Construct the command line string from our arguments\n",
    "training_args_command_line = \" \".join(f\"--{key} {value}\" for key, value in input_kwargs.items())\n",
    "\n",
    "# We execute the external script `run_lora_fine_tuning.py`\n",
    "if not os.path.exists(\"./logs\"):\n",
    "    os.makedirs(\"./logs\")\n",
    "train_log_file = f\"./logs/train_log_{input_kwargs['output_dir'].split('/')[-1]}.txt\"\n",
    "command = f\"./run_lora_fine_tuning.py {training_args_command_line} > {train_log_file}\"\n",
    "\n",
    "print(f\"Training log will be saved in {train_log_file}\")\n",
    "\n",
    "\n",
    "def is_port_open(host, port, timeout=1):\n",
    "    # Create a new socket using the with statement to ensure it's closed automatically\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        sock.settimeout(timeout)\n",
    "        result = sock.connect_ex((host, int(port)))\n",
    "        if result == 0:\n",
    "            sock.shutdown(socket.SHUT_RDWR)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "MASTER_PORT = 29500\n",
    "while is_port_open(\"localhost\", MASTER_PORT):\n",
    "    MASTER_PORT += 1\n",
    "\n",
    "distributed_runner = DistributedRunner(\n",
    "    command_list=[command], \n",
    "    world_size=1,        # Set to >1 for multi-card training\n",
    "    use_deepspeed=True,  # Enable DeepSpeed\n",
    "    master_port=MASTER_PORT\n",
    ")\n",
    "\n",
    "# Launch\n",
    "print(\"=\"*100)\n",
    "print(f\"Starting training with command: {command}\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Training log will be saved in {train_log_file}\")\n",
    "print(\"=\"*100)\n",
    "ret_code = distributed_runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199b12c",
   "metadata": {},
   "source": [
    "# 5. Evaluation\n",
    "Now that the model is trained, we verify its performance. We will use a helper function `evaluate` to generate text using both the base model and the fine-tuned adpater.\n",
    "\n",
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc861da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from optimum.habana.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(prompt_list, tokenizer, base_model, trained_model, trained_model2=None):\n",
    "    # Define the chat template used for fine-tuning\n",
    "    chat_template = (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{input}\\n\\n### Response:\\n\"\n",
    "    )\n",
    "    if isinstance(prompt_list, str):\n",
    "        prompt_list = [prompt_list]\n",
    "    for prompt in prompt_list:\n",
    "        # Generate the prompt for the base model using the original chat template\n",
    "        base_prompt = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True, enable_thinking=False\n",
    "        )\n",
    "        base_prompt = tokenizer.encode(base_prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        base_prompt = base_prompt.to(\"hpu\")\n",
    "\n",
    "        # Generate the prompt for the fine-tuned model using the new chat template\n",
    "        trained_prompt = chat_template.format(input=prompt)\n",
    "        trained_prompt = tokenizer.encode(trained_prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        trained_prompt = trained_prompt.to(\"hpu\")\n",
    "\n",
    "        # Generate the output for the base model\n",
    "        base_output = base_model.generate(\n",
    "            input_ids=base_prompt,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "        )\n",
    "        base_result = tokenizer.decode(base_output[0][len(base_prompt[0]) :], skip_special_tokens=True)\n",
    "\n",
    "        # Generate the output for the fine-tuned model\n",
    "        trained_output = trained_model.generate(\n",
    "            input_ids=trained_prompt,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "        )\n",
    "        trained_result = tokenizer.decode(trained_output[0][len(trained_prompt[0]) :], skip_special_tokens=True)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"=\" * 26 + \" Input \" + \"=\" * 26 + \"\\n\")\n",
    "        print(prompt + \"\\n\")\n",
    "        print(\"=\" * 21 + \" Original Output \" + \"=\" * 21 + \"\\n\")\n",
    "        print(base_result + \"\\n\")\n",
    "        print(\"=\" * 20 + \" Fine-tuned Output \" + \"=\" * 20 + \"\\n\")\n",
    "        print(trained_result + \"\\n\")\n",
    "\n",
    "        if trained_model2 is not None:\n",
    "            trained_output2 = trained_model2.generate(\n",
    "                input_ids=trained_prompt,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False,\n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                top_k=None,\n",
    "            )\n",
    "            trained_result2 = tokenizer.decode(trained_output2[0][len(trained_prompt[0]) :], skip_special_tokens=True)\n",
    "            print(\"=\" * 20 + \" Fine-tuned Output2 \" + \"=\" * 19 + \"\\n\")\n",
    "            print(trained_result2 + \"\\n\")\n",
    "\n",
    "        print(\"=\" * 27 + \" End \" + \"=\" * 27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1eb329",
   "metadata": {},
   "source": [
    "### Test and Enjoy! \n",
    "We load the tokenizer, the base model, and the fine-tuned LoRA adapters to prepare for evaluation. Then, compile each model using `torch.compile` for faster inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d27975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(input_kwargs[\"model_name_or_path\"])\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(input_kwargs[\"model_name_or_path\"])\n",
    "base_model = base_model.to(\"hpu\")\n",
    "base_model = torch.compile(base_model, backend=\"hpu_backend\")\n",
    "\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(input_kwargs[\"output_dir\"])\n",
    "trained_model = PeftModel.from_pretrained(trained_model, lora_output_dir)\n",
    "trained_model.merge_and_unload()\n",
    "trained_model = trained_model.to(\"hpu\")\n",
    "trained_model = torch.compile(trained_model, backend=\"hpu_backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081dc527",
   "metadata": {},
   "source": [
    "Feel free to modify the input prompts below to experiment with various queries and observe how the fine-tuned model's responses differ from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad01a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a example prompts. You can change the input prompt to see different outputs.\n",
    "input_prompt_list = [\n",
    "    \"오늘 저녁 메뉴 추천해줘\",\n",
    "    \"오늘 행사 재미있었어\",\n",
    "    \"주말에 뭐하면 좋을까\",\n",
    "]\n",
    "evaluate(input_prompt_list, tokenizer, base_model, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f91d4a",
   "metadata": {},
   "source": [
    "## Next Steps: Exploring Recent PEFT Methods\n",
    "We are now ready to move on to the next session to experiment with the latest PEFT techniques. Please restart the kernel to release resources and open the next [notebook](./2_GraLoRA_finetuning.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
