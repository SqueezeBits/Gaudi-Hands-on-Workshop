{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc1d259d",
   "metadata": {},
   "source": [
    "# [C-2]. Fine-tuning LLM with Intel® Gaudi®: GraLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1356f1f",
   "metadata": {},
   "source": [
    "# 1. Introducing GraLoRA (Granular Low-Rank Adaptation)\n",
    "\n",
    "<img src=\"./figures/gralora_overview.png\" width=\"75%\" height=\"75%\"/>\n",
    "\n",
    "In this session, we will explore [GraLoRA](https://huggingface.co/docs/peft/package_reference/gralora), a new PEFT (Parameter-Efficient FIne-tuning) method developed by our team.\n",
    "\n",
    "**[Important Note]**: Before you start, make sure to *restart kernel* in the previous notebook to release allocated resources.\n",
    "\n",
    "### What is GraLoRA?\n",
    "\n",
    "GraLoRA (Granular Low-Rank Adaptation) enhances the standard LoRA approach by using multiple small adapters to approximate the gradient of the full model. This method aims to be more robust and expressive than standard LoRA without additional costs.\n",
    "\n",
    "Since GraLoRA is fully integrated into the Hugging Face PEFT library, adapting our previous LoRA workflow is simple: we only need to change the `peft_type` to `gralora` and update our output directory.\n",
    "\n",
    "# 2. Configuration\n",
    "\n",
    "We will define the training arguments. The setup remains largely similar to the standard LoRA configuration, with the key difference being the `peft_type`.\n",
    "\n",
    "Key Settings:\n",
    "- peft_type: Set to `gralora`.\n",
    "- gaudi_config_name: Uses `Habana/qwen` for Gaudi  optimization.\n",
    "\n",
    "### 1. Determine the PEFT Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define PEFT (LoRA) Parameters\n",
    "input_kwargs = {\n",
    "    \"peft_type\": \"gralora\",\n",
    "    \"lora_rank\": 64,\n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eafdd16",
   "metadata": {},
   "source": [
    "### 2. Determine Training and Gaudi Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define PEFT (LoRA) Parameters\n",
    "input_kwargs = {\n",
    "    \"peft_type\": \"gralora\",\n",
    "    \"lora_rank\": 64,\n",
    "    \"lora_alpha\": 128,\n",
    "    \"lora_dropout\": 0.05,\n",
    "}\n",
    "\n",
    "# 2. Define Standard Transformer Training Arguments\n",
    "transformer_kwargs = {\n",
    "    \"model_name_or_path\": \"Qwen/Qwen3-0.6B\",\n",
    "    \"num_train_epochs\": 20,\n",
    "    \"seed\": 42,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"per_device_train_batch_size\": 256,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"logging_steps\": 1,\n",
    "}\n",
    "\n",
    "# Merge arguments\n",
    "input_kwargs.update(transformer_kwargs)\n",
    "\n",
    "# Generate a unique output directory name based on hyperparameters\n",
    "name_args = [\"peft_type\", \"lora_rank\", \"lora_alpha\", \"num_train_epochs\", \"learning_rate\"]\n",
    "gralora_output_dir = f\"./finetuned_models/joseon_persona_qwen3_0.6b\"\n",
    "for arg in name_args:\n",
    "    gralora_output_dir += f\"_{arg.replace('lora_', '')}-{input_kwargs[arg]}\"\n",
    "    \n",
    "print(f\"Model will be saved in: {gralora_output_dir}\")\n",
    "input_kwargs[\"output_dir\"] = gralora_output_dir\n",
    "\n",
    "# 3. Define Gaudi Specific Arguments\n",
    "gaudi_kwargs = {\n",
    "    \"use_habana\": True,  # Whether to use Gaudi s or not\n",
    "    \"use_lazy_mode\": True,  # Whether to use lazy or eager mode\n",
    "    \"gaudi_config_name\": \"Habana/qwen\",  # Gaudi configuration to use\n",
    "}\n",
    "input_kwargs.update(gaudi_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d485851b",
   "metadata": {},
   "source": [
    "# 3. Training Execution\n",
    "We use the DistributedRunner to execute the training script (`run_lora_fine_tuning.py`) on the Gaudi. We also enable DeepSpeed to optimize memory usage during training.\n",
    "\n",
    "- The training log will be saved in the `./logs` directory.\n",
    "- The loss curve will be saved in the `./train_loss` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51910eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from optimum.habana.distributed import DistributedRunner\n",
    "\n",
    "# Add DeepSpeed config to arguments\n",
    "input_kwargs[\"deepspeed\"] = \"configs/deepspeed_zero_1.json\" \n",
    "\n",
    "# Construct the command line string from our arguments\n",
    "training_args_command_line = \" \".join(f\"--{key} {value}\" for key, value in input_kwargs.items())\n",
    "\n",
    "# We execute the external script `run_lora_fine_tuning.py`\n",
    "if not os.path.exists(\"./logs\"):\n",
    "    os.makedirs(\"./logs\")\n",
    "train_log_file = f\"./logs/train_log_{input_kwargs['output_dir'].split('/')[-1]}.txt\"\n",
    "command = f\"./run_lora_fine_tuning.py {training_args_command_line} > {train_log_file}\"\n",
    "\n",
    "print(f\"Training log will be saved in {train_log_file}\")\n",
    "\n",
    "def is_port_open(host, port, timeout=1):\n",
    "    # Create a new socket using the with statement to ensure it's closed automatically\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "        sock.settimeout(timeout)\n",
    "        result = sock.connect_ex((host, int(port)))\n",
    "        if result == 0:\n",
    "            sock.shutdown(socket.SHUT_RDWR)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "MASTER_PORT = 29500\n",
    "while is_port_open(\"localhost\", MASTER_PORT):\n",
    "    MASTER_PORT += 1\n",
    "\n",
    "# Instantiate the DistributedRunner\n",
    "distributed_runner = DistributedRunner(\n",
    "    command_list=[command], \n",
    "    world_size=1,        # Set to >1 for multi-card training\n",
    "    use_deepspeed=True,  # Enable DeepSpeed\n",
    "    master_port=MASTER_PORT\n",
    ")\n",
    "\n",
    "# Launch\n",
    "print(f\"Starting training with command: {command}\")\n",
    "print(f\"Training log will be saved in {train_log_file}\")\n",
    "ret_code = distributed_runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c7d279",
   "metadata": {},
   "source": [
    "# 4. Inference Setup\n",
    "\n",
    "Now that the model is trained, we need to evaluate it. We will define a Python function evaluate that generates responses from both the Base Model and the Fine-Tuned Model side-by-side.\n",
    "\n",
    "**Note on Templates**\n",
    "- Base Model: Uses the standard chat template via tokenizer.apply_chat_template.\n",
    "- Fine-Tuned Model: Uses a specific \"Instruction/Response\" format that matches the Joseon Persona dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3df43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from optimum.habana.utils import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(prompt_list, tokenizer, base_model, trained_model, trained_model2=None):\n",
    "    # Define the chat template used for fine-tuning\n",
    "    chat_template = (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{input}\\n\\n### Response:\\n\"\n",
    "    )\n",
    "    if isinstance(prompt_list, str):\n",
    "        prompt_list = [prompt_list]\n",
    "    for prompt in prompt_list:\n",
    "        # Generate the prompt for the base model using the original chat template\n",
    "        base_prompt = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True, enable_thinking=False\n",
    "        )\n",
    "        base_prompt = tokenizer.encode(base_prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        base_prompt = base_prompt.to(\"hpu\")\n",
    "\n",
    "        # Generate the prompt for the fine-tuned model using the new chat template\n",
    "        trained_prompt = chat_template.format(input=prompt)\n",
    "        trained_prompt = tokenizer.encode(trained_prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        trained_prompt = trained_prompt.to(\"hpu\")\n",
    "\n",
    "        # Generate the output for the base model\n",
    "        base_output = base_model.generate(\n",
    "            input_ids=base_prompt,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "        )\n",
    "        base_result = tokenizer.decode(base_output[0][len(base_prompt[0]) :], skip_special_tokens=True)\n",
    "\n",
    "        # Generate the output for the fine-tuned model\n",
    "        trained_output = trained_model.generate(\n",
    "            input_ids=trained_prompt,\n",
    "            max_new_tokens=50,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None,\n",
    "        )\n",
    "        trained_result = tokenizer.decode(trained_output[0][len(trained_prompt[0]) :], skip_special_tokens=True)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"=\" * 26 + \" Input \" + \"=\" * 26 + \"\\n\")\n",
    "        print(prompt + \"\\n\")\n",
    "        print(\"=\" * 21 + \" Original Output \" + \"=\" * 21 + \"\\n\")\n",
    "        print(base_result + \"\\n\")\n",
    "        print(\"=\" * 20 + \" Fine-tuned Output \" + \"=\" * 20 + \"\\n\")\n",
    "        print(trained_result + \"\\n\")\n",
    "\n",
    "        if trained_model2 is not None:\n",
    "            trained_output2 = trained_model2.generate(\n",
    "                input_ids=trained_prompt,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=False,\n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                top_k=None,\n",
    "            )\n",
    "            trained_result2 = tokenizer.decode(trained_output2[0][len(trained_prompt[0]) :], skip_special_tokens=True)\n",
    "            print(\"=\" * 20 + \" Fine-tuned Output2 \" + \"=\" * 19 + \"\\n\")\n",
    "            print(trained_result2 + \"\\n\")\n",
    "\n",
    "        print(\"=\" * 27 + \" End \" + \"=\" * 27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d21f2b3",
   "metadata": {},
   "source": [
    "# 5. Load and Compile Models\n",
    "We load the tokenizer and the base model onto the Gaudi. Then, we load the GraLoRA adapters from our output directory, fuse them into the base model for efficiency, and compile the model using `torch.compile` for faster inference. \n",
    "Due to the compilation, first few warmup steps might be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b1bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(input_kwargs[\"model_name_or_path\"])\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(input_kwargs[\"model_name_or_path\"])\n",
    "base_model = base_model.to(\"hpu\")\n",
    "base_model = torch.compile(base_model, backend=\"hpu_backend\")\n",
    "\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(input_kwargs[\"output_dir\"])\n",
    "trained_model = PeftModel.from_pretrained(trained_model, gralora_output_dir)\n",
    "trained_model.merge_and_unload()\n",
    "trained_model = trained_model.to(\"hpu\")\n",
    "trained_model = torch.compile(trained_model, backend=\"hpu_backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55abc7",
   "metadata": {},
   "source": [
    "# 6. Evaluation\n",
    "Finally, we test the model with specific prompts to verify if it has successfully learned the \"Joseon Dynasty\" persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3bd2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a example prompts.\n",
    "input_prompt_list = [\n",
    "    \"오늘 저녁 메뉴 추천해줘\",\n",
    "    \"오늘 행사 재미있었어\",\n",
    "    \"주말에 뭐하면 좋을까\",\n",
    "]\n",
    "evaluate(input_prompt_list, tokenizer, base_model, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8059b5f5",
   "metadata": {},
   "source": [
    "You can further optimize the model's performance by modifying the training arguments in **Step 2. Configuration**. \n",
    "\n",
    "Recommended hyperparameters to tune:\n",
    "- `num_train_epochs`: Increase for better learning (watch for overfitting).\n",
    "- `lora_rank`: Adjust the rank of the adapters.\n",
    "- `learning_rate`: Fine-tune the step size.\n",
    "- `peft_type`: Switch back to \"lora\" to compare results directly against GraLoRA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
