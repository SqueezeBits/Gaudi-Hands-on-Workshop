{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf32e299-9192-433f-b378-1216d93af548",
   "metadata": {},
   "source": [
    "# PyTorch Profiler on Intel Gaudi HPU\n",
    "\n",
    "## Overview\n",
    "\n",
    "The PyTorch Profiler is a powerful tool for analyzing and optimizing the performance of PyTorch models running on Intel Gaudi AI accelerators (HPU). It provides detailed insights into:\n",
    "\n",
    "- **CPU and HPU Activity**: Track execution time spent on both host CPU and the Gaudi device\n",
    "- **Kernel Utilization**: Monitor Tensor Processing Cores (TPC) and Matrix Multiplication Engine (MME) usage\n",
    "- **Memory Profiling**: Analyze HPU memory allocation patterns during training/inference\n",
    "- **Performance Recommendations**: Get actionable guidance for optimization\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### ProfilerActivity Types\n",
    "- `torch.profiler.ProfilerActivity.CPU`: Profiles CPU operations\n",
    "- `torch.profiler.ProfilerActivity.HPU`: Profiles Intel Gaudi HPU operations (replaces CUDA for GPU)\n",
    "\n",
    "### Profiler Schedule\n",
    "The profiler uses a schedule to control data collection:\n",
    "- `wait`: Number of steps to skip before profiling\n",
    "- `warmup`: Number of warmup steps (data collected but not recorded)\n",
    "- `active`: Number of steps to actively record\n",
    "- `repeat`: Number of times to repeat the cycle\n",
    "\n",
    "### mark_step()\n",
    "On Gaudi HPU, `htcore.mark_step()` is crucial as it triggers the execution of accumulated graph operations. This is essential for proper profiling as it ensures operations are actually executed on the device.\n",
    "\n",
    "## Trace Output\n",
    "The profiler generates JSON trace files that can be:\n",
    "1. Viewed in TensorBoard using `torch-tb-profiler` plugin\n",
    "2. Uploaded to **Perfetto** trace viewer for detailed analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c0b2b",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24587744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: hpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import habana_frameworks.torch.core as htcore\n",
    "import os\n",
    "\n",
    "# Create output directory for trace files\n",
    "os.makedirs('./profile_traces', exist_ok=True)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('hpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7871874",
   "metadata": {},
   "source": [
    "## Trace 1: Random Tensor Addition (A + B)\n",
    "\n",
    "This trace captures the profiling data for element-wise addition of two random tensors with shape (1024, 1024).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4740bee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Trace 1: Tensor Addition (A + B)\n",
      "Trace 1 completed! Saved to: ./profile_traces/addition.pt.trace.json\n"
     ]
    }
   ],
   "source": [
    "# Define matrix size\n",
    "MATRIX_SIZE = (1024, 1024)\n",
    "\n",
    "# Define profiler activities for CPU and HPU\n",
    "activities = [\n",
    "    torch.profiler.ProfilerActivity.CPU,\n",
    "    torch.profiler.ProfilerActivity.HPU\n",
    "]\n",
    "\n",
    "# Trace 1: Tensor Addition\n",
    "print(\"Starting Trace 1: Tensor Addition (A + B)\")\n",
    "\n",
    "# Create random tensors on HPU\n",
    "A = torch.randn(MATRIX_SIZE, dtype=torch.float32, device=device)\n",
    "B = torch.randn(MATRIX_SIZE, dtype=torch.float32, device=device)\n",
    "\n",
    "# Define output file path\n",
    "trace1_path = './profile_traces/addition.pt.trace.json'\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    schedule=torch.profiler.schedule(wait=0, warmup=5, active=5, repeat=1),\n",
    "    activities=activities,\n",
    "    on_trace_ready=lambda p: p.export_chrome_trace(trace1_path),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as profiler:\n",
    "    for step in range(15):        \n",
    "        # Perform addition\n",
    "        C = A + B\n",
    "        \n",
    "        # Trigger execution of accumulated operations\n",
    "        htcore.mark_step()\n",
    "        \n",
    "        # Step the profiler\n",
    "        profiler.step()\n",
    "\n",
    "print(f\"Trace 1 completed! Saved to: {trace1_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a91aa7",
   "metadata": {},
   "source": [
    "## Trace 2: Random Tensor Matrix Multiplication (A @ B)\n",
    "\n",
    "This trace captures the profiling data for matrix multiplication of two random tensors with shape (1024, 1024). Matrix multiplication utilizes the Matrix Multiplication Engine (MME) on Gaudi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ad0178d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Trace 2: Matrix Multiplication (A @ B)\n",
      "Trace 2 completed! Saved to: ./profile_traces/matmul.pt.trace.json\n"
     ]
    }
   ],
   "source": [
    "# Trace 2: Matrix Multiplication\n",
    "print(\"Starting Trace 2: Matrix Multiplication (A @ B)\")\n",
    "\n",
    "# Create random tensors on HPU\n",
    "A = torch.randn(MATRIX_SIZE, dtype=torch.float32, device=device)\n",
    "B = torch.randn(MATRIX_SIZE, dtype=torch.float32, device=device)\n",
    "\n",
    "# Define output file path\n",
    "trace2_path = './profile_traces/matmul.pt.trace.json'\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    schedule=torch.profiler.schedule(wait=0, warmup=5, active=5, repeat=1),\n",
    "    activities=activities,\n",
    "    on_trace_ready=lambda p: p.export_chrome_trace(trace2_path),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as profiler:\n",
    "    for step in range(15):\n",
    "        # Perform matrix multiplication\n",
    "        C = A @ B  # equivalent to torch.matmul(A, B)\n",
    "        \n",
    "        # Trigger execution of accumulated operations\n",
    "        htcore.mark_step()\n",
    "        \n",
    "        # Step the profiler\n",
    "        profiler.step()\n",
    "\n",
    "print(f\"Trace 2 completed! Saved to: {trace2_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07b8558",
   "metadata": {},
   "source": [
    "## Trace 3: Matmul ‚Üí ReLU ‚Üí Matmul (Eager Mode vs. torch.compile)\n",
    "\n",
    "This trace compares the performance of a simple neural network pattern (matmul ‚Üí relu ‚Üí matmul) between:\n",
    "\n",
    "1. **Eager Mode**: Direct execution without graph compilation\n",
    "2. **torch.compile with hpu_backend**: Intel Gaudi optimized graph compilation\n",
    "\n",
    "According to [Habana Documentation](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Getting_Started_with_Inference.html), `torch.compile` with `hpu_backend` is the recommended approach for optimal performance on Intel Gaudi. The backend enables graph-level optimizations specific to HPU architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22b3aa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created for Eager mode profiling\n",
      "MatmulReluMatmul()\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a simple model: Matmul -> ReLU -> Matmul\n",
    "class MatmulReluMatmul(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.weight1 = nn.Parameter(torch.randn(size, size))\n",
    "        self.weight2 = nn.Parameter(torch.randn(size, size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Step 1: Matmul\n",
    "        x = torch.matmul(x, self.weight1)\n",
    "        # Step 2: ReLU\n",
    "        x = F.relu(x)\n",
    "        # Step 3: Matmul\n",
    "        x = torch.matmul(x, self.weight2)\n",
    "        return x\n",
    "\n",
    "# Create model for Eager mode (no torch.compile)\n",
    "model_eager = MatmulReluMatmul(MATRIX_SIZE[0]).to(device)\n",
    "model_eager.eval()\n",
    "\n",
    "print(\"Model created for Eager mode profiling\")\n",
    "print(model_eager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec0948f",
   "metadata": {},
   "source": [
    "### Trace 3-a: Eager Mode (without torch.compile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "318da500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Trace 3a: Matmul -> ReLU -> Matmul (Eager Mode)\n",
      "Trace 3a completed! Saved to: ./profile_traces/eager_mode.pt.trace.json\n"
     ]
    }
   ],
   "source": [
    "# Trace 3a: Eager Mode\n",
    "print(\"Starting Trace 3a: Matmul -> ReLU -> Matmul (Eager Mode)\")\n",
    "\n",
    "# Create input tensor\n",
    "input_tensor = torch.randn(MATRIX_SIZE, dtype=torch.float32, device=device)\n",
    "\n",
    "# Define output file path\n",
    "trace3a_path = './profile_traces/eager_mode.pt.trace.json'\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=0, warmup=5, active=5, repeat=1),\n",
    "        activities=activities,\n",
    "        on_trace_ready=lambda p: p.export_chrome_trace(trace3a_path),\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as profiler:\n",
    "        for step in range(15):\n",
    "            # Forward pass: Matmul -> ReLU -> Matmul\n",
    "            output = model_eager(input_tensor)\n",
    "            \n",
    "            # Trigger execution of accumulated operations\n",
    "            htcore.mark_step()\n",
    "            \n",
    "            # Step the profiler\n",
    "            profiler.step()\n",
    "\n",
    "print(f\"Trace 3a completed! Saved to: {trace3a_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c59334",
   "metadata": {},
   "source": [
    "### Trace 3-b: torch.compile with hpu_backend\n",
    "\n",
    "Using `torch.compile(model, backend=\"hpu_backend\")` enables Intel Gaudi-specific graph optimizations for improved performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e07b4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled with hpu_backend\n",
      "Starting Trace 3b: Matmul -> ReLU -> Matmul (torch.compile)\n",
      "Trace 3b completed! Saved to: ./profile_traces/torch_compile.pt.trace.json\n"
     ]
    }
   ],
   "source": [
    "# Create a new model instance for torch.compile\n",
    "model_compiled = MatmulReluMatmul(MATRIX_SIZE[0]).to(device)\n",
    "model_compiled.eval()\n",
    "\n",
    "# Wrap with torch.compile using hpu_backend\n",
    "model_compiled = torch.compile(model_compiled, backend=\"hpu_backend\")\n",
    "\n",
    "print(\"Model compiled with hpu_backend\")\n",
    "\n",
    "# Trace 3b: torch.compile Mode\n",
    "print(\"Starting Trace 3b: Matmul -> ReLU -> Matmul (torch.compile)\")\n",
    "\n",
    "# Create input tensor\n",
    "input_tensor = torch.randn(MATRIX_SIZE, dtype=torch.float32, device=device)\n",
    "\n",
    "# Define output file path\n",
    "trace3b_path = './profile_traces/torch_compile.pt.trace.json'\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.profiler.profile(\n",
    "        schedule=torch.profiler.schedule(wait=0, warmup=5, active=5, repeat=1),\n",
    "        activities=activities,\n",
    "        on_trace_ready=lambda p: p.export_chrome_trace(trace3b_path),\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as profiler:\n",
    "        for step in range(15):\n",
    "            # Forward pass: Matmul -> ReLU -> Matmul (compiled)\n",
    "            output = model_compiled(input_tensor)\n",
    "            \n",
    "            # Trigger execution of accumulated operations\n",
    "            htcore.mark_step()\n",
    "            \n",
    "            # Step the profiler\n",
    "            profiler.step()\n",
    "\n",
    "print(f\"Trace 3b completed! Saved to: {trace3b_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca967b6",
   "metadata": {},
   "source": [
    "## List Generated Trace Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "195aba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Generated Trace Files Summary\n",
      "============================================================\n",
      "\n",
      "‚úÖ Trace 1 - Addition (A + B):\n",
      "   ./profile_traces/addition.pt.trace.json (171.2 KB)\n",
      "\n",
      "‚úÖ Trace 2 - Matrix Multiplication (A @ B):\n",
      "   ./profile_traces/matmul.pt.trace.json (137.2 KB)\n",
      "\n",
      "‚úÖ Trace 3a - Matmul‚ÜíReLU‚ÜíMatmul (Eager Mode):\n",
      "   ./profile_traces/eager_mode.pt.trace.json (282.7 KB)\n",
      "\n",
      "‚úÖ Trace 3b - Matmul‚ÜíReLU‚ÜíMatmul (torch.compile):\n",
      "   ./profile_traces/torch_compile.pt.trace.json (581.4 KB)\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Generated Trace Files Summary\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trace_files = [\n",
    "    (\"Trace 1 - Addition (A + B)\", \"./profile_traces/addition.pt.trace.json\"),\n",
    "    (\"Trace 2 - Matrix Multiplication (A @ B)\", \"./profile_traces/matmul.pt.trace.json\"),\n",
    "    (\"Trace 3a - Matmul‚ÜíReLU‚ÜíMatmul (Eager Mode)\", \"./profile_traces/eager_mode.pt.trace.json\"),\n",
    "    (\"Trace 3b - Matmul‚ÜíReLU‚ÜíMatmul (torch.compile)\", \"./profile_traces/torch_compile.pt.trace.json\"),\n",
    "]\n",
    "\n",
    "for name, path in trace_files:\n",
    "    exists = \"‚úÖ\" if os.path.exists(path) else \"‚ùå\"\n",
    "    size = f\"({os.path.getsize(path) / 1024:.1f} KB)\" if os.path.exists(path) else \"(not found)\"\n",
    "    print(f\"\\n{exists} {name}:\")\n",
    "    print(f\"   {path} {size}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec030009",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Viewing the Trace Results\n",
    "\n",
    "### Habana Perfetto Viewer\n",
    "\n",
    "**Upload the file to https://perfetto.habana.ai and view the API calls and hardware trace events.**\n",
    "\n",
    "Steps:\n",
    "1. Download the generated `.json` trace files to your local machine:\n",
    "   - `./profile_traces/addition.pt.trace.json` - Tensor Addition trace\n",
    "   - `./profile_traces/matmul.pt.trace.json` - Matrix Multiplication trace\n",
    "   - `./profile_traces/eager_mode.pt.trace.json` - Eager Mode (Matmul‚ÜíReLU‚ÜíMatmul)\n",
    "   - `./profile_traces/torch_compile.pt.trace.json` - torch.compile Mode (Matmul‚ÜíReLU‚ÜíMatmul)\n",
    "2. Open https://perfetto.habana.ai in your browser\n",
    "3. Click \"Open trace file\" and select your downloaded JSON file\n",
    "4. Explore the trace timeline to analyze:\n",
    "   - CPU and HPU activity\n",
    "   - Kernel execution times (TPC and MME utilization)\n",
    "   - Memory operations\n",
    "   - API call sequences\n",
    "\n",
    "### Comparing Eager Mode vs. torch.compile\n",
    "\n",
    "When viewing Trace 3a (Eager Mode) and Trace 3b (torch.compile), compare:\n",
    "- **Graph compilation overhead**: torch.compile may have initial compilation cost\n",
    "- **Kernel fusion**: torch.compile with `hpu_backend` can fuse multiple operations\n",
    "- **Overall execution time**: Compiled graphs typically run faster after warmup\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
