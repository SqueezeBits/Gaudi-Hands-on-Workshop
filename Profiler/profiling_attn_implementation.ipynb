{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7cd6297",
   "metadata": {},
   "source": [
    "# Profiling Different Attention Implementations on Intel Gaudi HPU\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to profile and compare different attention mechanism implementations on Intel Gaudi AI accelerators. Attention is the core component of Transformer models, and its performance is critical for LLM inference.\n",
    "\n",
    "We compare three attention implementations:\n",
    "\n",
    "1. **Fused SDPA (Scaled Dot-Product Attention)**: Uses Habana's optimized `FusedSDPA` kernel for maximum performance\n",
    "2. **Initial PagedAttention**: Loop-based attention with fetching cache using index select\n",
    "3. **Flat PagedAttention**: paged attention using flattened layout removing internal fragmentation\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### PagedAttention\n",
    "PagedAttention is a memory-efficient attention mechanism that stores KV cache in non-contiguous memory blocks, similar to virtual memory paging. This enables:\n",
    "- Efficient memory utilization for variable-length sequences\n",
    "- Better batching of requests with different context lengths\n",
    "- Reduced memory fragmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e809e5f",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Define Helper Functions\n",
    "\n",
    "This section sets up:\n",
    "- **Profiler configuration**: Wait, warmup, and active step counts\n",
    "- **Attention parameters**: Number of heads, head dimension\n",
    "- **Helper functions**: Synchronization, profiler setup, benchmarking utilities\n",
    "- **Input Data Preparation**: Functions for preparing inputs for each attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922140d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import os\n",
    "import time\n",
    "\n",
    "os.environ['PT_HPU_LAZY_MODE'] = '1'\n",
    "os.environ['HABANA_PROFILE'] = 'profile_api'\n",
    "\n",
    "import habana_frameworks.torch.hpu as ht\n",
    "import habana_frameworks.torch as htorch\n",
    "from habana_frameworks.torch.hpex.kernels import FusedSDPA\n",
    "import torch\n",
    "\n",
    "PROF_WAIT = 0\n",
    "PROF_WARMUP = 1\n",
    "PROF_ACTIVE = 3\n",
    "STEPS = PROF_WARMUP + PROF_ACTIVE\n",
    "DEVICE = os.environ.get('DEVICE', 'hpu')\n",
    "\n",
    "MAX_BLOCKS = 1024\n",
    "\n",
    "Q_NUM_HEADS = 32\n",
    "K_NUM_HEADS = 32\n",
    "\n",
    "HEAD_DIM = 128\n",
    "BLOCK_SIZE = 128\n",
    "PA_SPLIT_VALUE = 1\n",
    "\n",
    "INDEX_DTYPE = torch.int32 if DEVICE == 'hpu' else torch.int64\n",
    "\n",
    "Scenario = namedtuple(\"Scenario\", [\"name\", \"fn\", \"data_fn\", \"params\"])\n",
    "\n",
    "def sync():\n",
    "    if DEVICE == 'hpu':\n",
    "        htorch.core.mark_step()\n",
    "        htorch.hpu.synchronize()\n",
    "\n",
    "def pt_profiler(schedule, attn_name):\n",
    "    activities = [torch.profiler.ProfilerActivity.CPU]\n",
    "    activities.extend([torch.profiler.ProfilerActivity.HPU] if DEVICE == 'hpu' else [])\n",
    "\n",
    "    profiler = torch.profiler.profile(\n",
    "        schedule=schedule,\n",
    "        activities=activities,\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler(f'./{attn_name}', use_gzip=True),\n",
    "        record_shapes=False,\n",
    "        with_stack=True)\n",
    "    return profiler\n",
    "\n",
    "def setup_profiler(method, attn_name):\n",
    "    schedule = torch.profiler.schedule(wait=PROF_WAIT, warmup=PROF_WARMUP, active=PROF_ACTIVE, repeat=1)\n",
    "    return method(schedule, attn_name)\n",
    "\n",
    "def params_str(params):\n",
    "    sep = ' '\n",
    "    return sep.join(f'{k}:{v}' for k, v in params.items() if k != 'context_lens')\n",
    "\n",
    "def define(*args, **params):\n",
    "    return Scenario(*args, params=params)\n",
    "\n",
    "def run(step, scenario, data):\n",
    "    name = f'{scenario.name} | {params_str(scenario.params)}'\n",
    "    args, kwargs = data\n",
    "    start_ts = time.perf_counter()\n",
    "    res = scenario.fn(*args, **kwargs)\n",
    "    sync()\n",
    "    diff_ts = 1000 * (time.perf_counter() - start_ts)\n",
    "    print(f'{step} | {name} | {diff_ts:.3f}ms')\n",
    "    return diff_ts\n",
    "\n",
    "def benchmark(scenarios):\n",
    "    data = [s.data_fn(**s.params) for s in scenarios]\n",
    "    sync()\n",
    "    for s, d in zip(scenarios, data):\n",
    "        prof = setup_profiler(pt_profiler, s.name)\n",
    "        prof.start()\n",
    "        for i in range(STEPS):\n",
    "            sync()\n",
    "            run(i, s, d)\n",
    "            prof.step()\n",
    "        prof.stop()\n",
    "\n",
    "def round_up(x, k):\n",
    "    return (x + k - 1) // k * k\n",
    "\n",
    "def fetch_from_cache(cache, blocks, permutations):\n",
    "    if permutations is not None:\n",
    "        return [cache.index_select(0, blocks[:, i]).permute(permutations) for i in range(blocks.size(1))]\n",
    "    else:\n",
    "        return [cache.index_select(0, blocks[:, i]) for i in range(blocks.size(1))]\n",
    "\n",
    "def flat_pa_data(batch_size, num_blocks):\n",
    "    cache_shape = (MAX_BLOCKS, BLOCK_SIZE, K_NUM_HEADS, HEAD_DIM)\n",
    "    params = {\n",
    "        'query': torch.empty(batch_size, Q_NUM_HEADS, HEAD_DIM, dtype=torch.bfloat16, device=DEVICE),\n",
    "        'key_cache': torch.empty(*cache_shape, dtype=torch.bfloat16, device=DEVICE),\n",
    "        'value_cache': torch.empty(*cache_shape, dtype=torch.bfloat16, device=DEVICE),\n",
    "        'scale': 1.0,\n",
    "        'block_list': torch.zeros(num_blocks, dtype=INDEX_DTYPE, device=DEVICE),\n",
    "        'block_mapping': torch.zeros(num_blocks, batch_size, dtype=torch.bfloat16, device=DEVICE),\n",
    "        'block_bias': torch.zeros(num_blocks, BLOCK_SIZE, dtype=torch.bool, device=DEVICE),\n",
    "    }\n",
    "    return (), params\n",
    "\n",
    "def pa_v0_2_data(batch_size, seq_len, context_lens=None):\n",
    "    num_blocks = round_up(seq_len, BLOCK_SIZE) // BLOCK_SIZE\n",
    "    cache_shape = (MAX_BLOCKS, K_NUM_HEADS, HEAD_DIM, BLOCK_SIZE)\n",
    "    params = {\n",
    "        'query': torch.empty(batch_size, Q_NUM_HEADS, HEAD_DIM, dtype=torch.bfloat16, device=DEVICE),\n",
    "        'key_cache': torch.empty(*cache_shape, dtype=torch.bfloat16, device=DEVICE),\n",
    "        'value_cache': torch.empty(*cache_shape, dtype=torch.bfloat16, device=DEVICE),\n",
    "        'head_mapping': None,\n",
    "        'scale': 1.0,\n",
    "        'block_tables': torch.zeros(batch_size, num_blocks, dtype=INDEX_DTYPE, device=DEVICE),\n",
    "        'context_lens': torch.ones(batch_size, dtype=INDEX_DTYPE, device=DEVICE) * seq_len if context_lens is None else context_lens,\n",
    "        'block_size': BLOCK_SIZE,\n",
    "        'alibi_slopes': None,\n",
    "    }\n",
    "    return (), params\n",
    "\n",
    "def fetch_from_cache_pa_v0_3(cache, blocks):\n",
    "    return [cache.index_select(0, blocks[:, i]) for i in range(blocks.size(1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb25c845",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Attention Implementation 1: Fused SDPA\n",
    "\n",
    "### Fused Scaled Dot-Product Attention\n",
    "\n",
    "Habana's `FusedSDPA` is a highly optimized kernel that fuses multiple attention operations into a single efficient kernel:\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(Q @ K^T / ‚àöd_k) @ V\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "- Single fused kernel reduces memory bandwidth requirements\n",
    "- Optimized for Intel Gaudi's Matrix Multiplication Engine (MME)\n",
    "- Best performance for prefill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attn_baseline(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    scale: float,\n",
    "    attn_mask: None,\n",
    ") -> torch.Tensor:\n",
    "    with ht.sdp_kernel(enable_recompute = False):\n",
    "        sdpa_out = FusedSDPA.apply(query, key, value, None, 0.0, False)\n",
    "    return sdpa_out\n",
    "\n",
    "def attn_baseline_data(batch_size, seq_len):\n",
    "    params = {\n",
    "        'query': torch.ones(batch_size, Q_NUM_HEADS, 1, HEAD_DIM, dtype=torch.bfloat16, device=DEVICE),\n",
    "        'key': torch.ones(batch_size, K_NUM_HEADS, seq_len, HEAD_DIM, dtype=torch.bfloat16, device=DEVICE),\n",
    "        'value': torch.ones(batch_size, K_NUM_HEADS, seq_len, HEAD_DIM, dtype=torch.bfloat16, device=DEVICE),\n",
    "        'scale': 1.0,\n",
    "        'attn_mask': None\n",
    "        \n",
    "    }\n",
    "    return (), params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfaafae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Attention Implementation 2: Inital PagedAttention\n",
    "\n",
    "### Initial PagedAttention\n",
    "\n",
    "**Algorithm:**\n",
    "1. Create attention mask based on `context_lens` for variable-length sequences\n",
    "2. Fetch Key blocks from cache using `block_tables`\n",
    "3. Compute attention scores: `Q @ K^T` for each block\n",
    "4. Apply masking and softmax across all blocks\n",
    "5. Fetch Value blocks and compute weighted sum\n",
    "\n",
    "**Key Features:**\n",
    "- index_selct based cache fetching\n",
    "- loop based attention computation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed57522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pa_v0_3(query, key_cache, value_cache, head_mapping, scale, block_tables, context_lens, block_size, alibi_slopes, attn_masks=None)  -> None:\n",
    "    if alibi_slopes is not None:\n",
    "        raise NotImplementedError\n",
    "    if attn_masks is not None:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    seq_len = block_tables.size(1)\n",
    "    batch_size, query_heads, _ = query.shape\n",
    "    _, kv_heads, _, _ = key_cache.shape\n",
    "    min_inf = torch.finfo(query.dtype).min\n",
    "    mask = (torch.arange(0, seq_len * block_size, dtype=torch.int32, device=key_cache.device)\n",
    "            .view(1, -1)\n",
    "            .expand(batch_size, -1)\n",
    "            .ge(context_lens.view(-1, 1))\n",
    "            .view(batch_size, 1, 1, -1))\n",
    "    query = query.unsqueeze(-2)\n",
    "    keys = fetch_from_cache_pa_v0_3(key_cache, block_tables)\n",
    "    if query_heads != kv_heads:\n",
    "        query = query.unflatten(1, (kv_heads, -1))\n",
    "        keys = [k.unflatten(1, (kv_heads, 1)) for k in keys]\n",
    "        mask = mask.unsqueeze(2)\n",
    "\n",
    "    attn_weights = [torch.matmul(query, k) for k in keys]\n",
    "    attn_weights = (torch.cat(attn_weights, dim=-1)\n",
    "                    .mul_(scale)\n",
    "                    .masked_fill(mask, min_inf)\n",
    "                    .softmax(dim=-1))\n",
    "\n",
    "    values = fetch_from_cache_pa_v0_3(value_cache, block_tables)\n",
    "    if PA_SPLIT_VALUE:\n",
    "        attn_weights = attn_weights.split(block_size, dim=-1)\n",
    "    else:\n",
    "        values = [torch.cat(values, dim=-1)]\n",
    "        attn_weights = [attn_weights]\n",
    "    if query_heads != kv_heads:\n",
    "        values = [v.unflatten(1, (kv_heads, 1)) for v in values]\n",
    "    attn_weights = [torch.matmul(a, v.transpose(-1, -2)).squeeze(-2) for a, v in zip(attn_weights, values)]\n",
    "    if query_heads != kv_heads:\n",
    "        attn_weights = [a.flatten(1, 2) for a in attn_weights]\n",
    "    attn_weights = sum(attn_weights)\n",
    "\n",
    "    return attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af21ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Attention Implementation 3: Flat PagedAttention\n",
    "\n",
    "### Flat PagedAttention\n",
    "\n",
    "This implementation uses a \"flattened\" approach where batch and block dimensions are merged, removing internal fragmentation within a batch.\n",
    "\n",
    "**Key Operations:**\n",
    "- `batch2block`: Transforms batch-indexed tensors to block-indexed using `block_mapping`\n",
    "- `block2batch`: Inverse transformation from block to batch indexing\n",
    "- `block_softmax`: Custom softmax that correctly normalizes across blocks belonging to the same batch item\n",
    "\n",
    "**Algorithm:**\n",
    "1. Transform query from batch space to block space\n",
    "2. Fetch Key/Value from cache using `block_list`\n",
    "3. Compute attention scores with block-level bias\n",
    "4. Apply custom `block_softmax` for correct normalization\n",
    "5. Compute weighted sum and transform back to batch space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6badb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch2block(tensor, block_mapping):\n",
    "    shape = tuple(tensor.shape)\n",
    "    return (block_mapping @ tensor.view(shape[0], -1)).view(-1, *shape[1:])\n",
    "\n",
    "def block2batch(tensor, block_mapping):\n",
    "    shape = tuple(tensor.shape)\n",
    "    return (block_mapping.t() @ tensor.view(shape[0], -1)).view(-1, *shape[1:])\n",
    "\n",
    "def block_softmax(batch_size, attn, block_mapping):\n",
    "    attn = attn.exp_()\n",
    "    sums = attn.sum(dim=-1).unsqueeze(-1)\n",
    "    sums = block2batch(sums, block_mapping)\n",
    "    sums = batch2block(sums, block_mapping)\n",
    "    attn.div_(sums)\n",
    "    return attn\n",
    "\n",
    "def flat_pa(query, key_cache, value_cache, block_list, block_mapping, block_bias, scale):\n",
    "    batch_size = query.size(0)\n",
    "    q_heads = query.size(1)\n",
    "    kv_heads = key_cache.size(2)\n",
    "\n",
    "    query = batch2block(scale * query, block_mapping).unsqueeze(-2)\n",
    "    key = torch.index_select(key_cache, 0, block_list).transpose(1, 2)\n",
    "    value = torch.index_select(value_cache, 0, block_list).transpose(1, 2)\n",
    "    block_bias = block_bias.view(key.size(0), 1, 1, -1)\n",
    "\n",
    "    if kv_heads != q_heads:\n",
    "        block_bias = block_bias.unsqueeze(1)\n",
    "        query = query.unflatten(1, (kv_heads, -1))\n",
    "        key = key.unflatten(1, (kv_heads, 1))\n",
    "        value = value.unflatten(1, (kv_heads, 1))\n",
    "        key = key.transpose(3, 4)\n",
    "    else:\n",
    "        key = key.transpose(2, 3)\n",
    "\n",
    "    attn = (query @ key) + block_bias\n",
    "    attn = block_softmax(batch_size, attn, block_mapping)\n",
    "    attn = attn @ value\n",
    "    attn = block2batch(attn, block_mapping)\n",
    "    attn = attn.squeeze(-2)\n",
    "    if kv_heads != q_heads:\n",
    "        attn = attn.flatten(1, 2)\n",
    "    return attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ecdfc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run Profiling Benchmark\n",
    "\n",
    "### Benchmark Configuration\n",
    "\n",
    "The benchmark runs all three attention implementations with the following parameters:\n",
    "- **Batch size**: 32 sequences\n",
    "- **Sequence length**: 1024 tokens\n",
    "\n",
    "Each implementation will be profiled separately, and traces will be saved to individual directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9fb892",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 1024\n",
    "torch.manual_seed(42)\n",
    "context_lens = torch.randint(low=1, high=seq_len+1, size=(batch_size,), dtype=INDEX_DTYPE, device=DEVICE)\n",
    "num_blocks = int(torch.sum(torch.ceil(context_lens/BLOCK_SIZE)).item())\n",
    "scenarios = [\n",
    "    define('attn_baseline', attn_baseline, attn_baseline_data, batch_size=batch_size, seq_len=seq_len),\n",
    "    define('initial_pa', pa_v0_3, pa_v0_2_data, batch_size=batch_size, seq_len=seq_len, context_lens=context_lens),\n",
    "    define('flat_pa', flat_pa, flat_pa_data, batch_size=batch_size, num_blocks=num_blocks),\n",
    "]\n",
    "benchmark(scenarios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5998172",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Viewing the Trace Results\n",
    "\n",
    "### Generated Trace Files\n",
    "\n",
    "After running the benchmark, trace files are saved in the following directories:\n",
    "- `./attn_baseline/` - Fused SDPA traces\n",
    "- `./initial_pa/` - Initial PagedAttention traces\n",
    "- `./flat_pa/` - Flat PagedAttention traces\n",
    "\n",
    "### Habana Perfetto Viewer\n",
    "\n",
    "**Upload the file to https://perfetto.habana.ai and view the API calls and hardware trace events.**\n",
    "\n",
    "Steps:\n",
    "1. Download the generated `.json.gz` trace files to your local machine\n",
    "2. Open https://perfetto.habana.ai in your browser\n",
    "3. Click \"Open trace file\" and select your downloaded trace file\n",
    "4. Explore the trace timeline to analyze:\n",
    "   - CPU and HPU activity overlap\n",
    "   - Kernel execution times (TPC and MME utilization)\n",
    "   - Memory transfer operations\n",
    "   - Attention kernel performance differences\n",
    "\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "- **Fused SDPA** typically has the lowest latency for standard attention patterns\n",
    "- **PagedAttention variants** excel when memory efficiency is critical (e.g., long contexts)\n",
    "- Compare the **MME utilization** in traces to understand compute efficiency\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
